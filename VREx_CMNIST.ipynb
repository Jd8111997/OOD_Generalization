{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import torchvision.datasets.utils as dataset_utils"
      ],
      "metadata": {
        "id": "G_yDzgPcDnkY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_grayscale_arr(arr, red=True):\n",
        "  \"\"\"Converts grayscale image to either red or green\"\"\"\n",
        "  assert arr.ndim == 2\n",
        "  dtype = arr.dtype\n",
        "  h, w = arr.shape\n",
        "  arr = np.reshape(arr, [h, w, 1])\n",
        "  if red:\n",
        "    arr = np.concatenate([arr,\n",
        "                          np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
        "  else:\n",
        "    arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
        "                          arr,\n",
        "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  return arr\n",
        "\n",
        "\n",
        "class ColoredMNIST(datasets.VisionDataset):\n",
        "  \"\"\"\n",
        "  Colored MNIST dataset for testing IRM. Prepared using procedure from https://arxiv.org/pdf/1907.02893.pdf\n",
        "\n",
        "  Args:\n",
        "    root (string): Root directory of dataset where ``ColoredMNIST/*.pt`` will exist.\n",
        "    env (string): Which environment to load. Must be 1 of 'train1', 'train2', 'test', or 'all_train'.\n",
        "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "      and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "    target_transform (callable, optional): A function/transform that takes in the\n",
        "      target and transforms it.\n",
        "  \"\"\"\n",
        "  def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
        "    super(ColoredMNIST, self).__init__(root, transform=transform,\n",
        "                                target_transform=target_transform)\n",
        "\n",
        "    self.prepare_colored_mnist()\n",
        "    if env in ['train1', 'train2', 'test']:\n",
        "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
        "    elif env == 'all_train':\n",
        "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt')) + \\\n",
        "                               torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'))\n",
        "    else:\n",
        "      raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        index (int): Index\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image, target) where target is index of the target class.\n",
        "    \"\"\"\n",
        "    img, target = self.data_label_tuples[index]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_label_tuples)\n",
        "\n",
        "  def prepare_colored_mnist(self):\n",
        "    colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
        "    if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
        "        and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
        "        and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
        "      print('Colored MNIST dataset already exists')\n",
        "      return\n",
        "\n",
        "    print('Preparing Colored MNIST')\n",
        "    train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
        "\n",
        "    train1_set = []\n",
        "    train2_set = []\n",
        "    test_set = []\n",
        "    for idx, (im, label) in enumerate(train_mnist):\n",
        "      if idx % 10000 == 0:\n",
        "        print(f'Converting image {idx}/{len(train_mnist)}')\n",
        "      im_array = np.array(im)\n",
        "\n",
        "      # Assign a binary label y to the image based on the digit\n",
        "      binary_label = 0 if label < 5 else 1\n",
        "\n",
        "      # Flip label with 25% probability\n",
        "      if np.random.uniform() < 0.25:\n",
        "        binary_label = binary_label ^ 1\n",
        "\n",
        "      # Color the image either red or green according to its possibly flipped label\n",
        "      color_red = binary_label == 0\n",
        "\n",
        "      # Flip the color with a probability e that depends on the environment\n",
        "      if idx < 20000:\n",
        "        # 20% in the first training environment\n",
        "        if np.random.uniform() < 0.2:\n",
        "          color_red = not color_red\n",
        "      elif idx < 40000:\n",
        "        # 10% in the first training environment\n",
        "        if np.random.uniform() < 0.1:\n",
        "          color_red = not color_red\n",
        "      else:\n",
        "        # 90% in the test environment\n",
        "        if np.random.uniform() < 0.9:\n",
        "          color_red = not color_red\n",
        "\n",
        "      colored_arr = color_grayscale_arr(im_array, red=color_red)\n",
        "\n",
        "      if idx < 20000:\n",
        "        train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "      elif idx < 40000:\n",
        "        train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "      else:\n",
        "        test_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "\n",
        "      # Debug\n",
        "      # print('original label', type(label), label)\n",
        "      # print('binary label', binary_label)\n",
        "      # print('assigned color', 'red' if color_red else 'green')\n",
        "      # plt.imshow(colored_arr)\n",
        "      # plt.show()\n",
        "      # break\n",
        "\n",
        "    if not os.path.exists(colored_mnist_dir):\n",
        "      os.makedirs(colored_mnist_dir)\n",
        "    torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
        "    torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
        "    torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))"
      ],
      "metadata": {
        "id": "Dhz_1NdC88CD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dataset_digits(dataset):\n",
        "  fig = plt.figure(figsize=(13, 8))\n",
        "  columns = 6\n",
        "  rows = 3\n",
        "  # ax enables access to manipulate each of subplots\n",
        "  ax = []\n",
        "\n",
        "  for i in range(columns * rows):\n",
        "    img, label = dataset[i]\n",
        "    # create subplot and append to ax\n",
        "    ax.append(fig.add_subplot(rows, columns, i + 1))\n",
        "    ax[-1].set_title(\"Label: \" + str(label))  # set title\n",
        "    plt.imshow(img)\n",
        "\n",
        "  plt.show()  # finally, render the plot\n",
        "  "
      ],
      "metadata": {
        "id": "m6m7xgc29AYH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1_set = ColoredMNIST(root='./data', env='train1')\n",
        "plot_dataset_digits(train1_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "h-Z6uazD9ENh",
        "outputId": "e35c10ff-15bb-47d5-c5d4-8badf7f431e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colored MNIST dataset already exists\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 936x576 with 18 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = ColoredMNIST(root='./data', env='test')\n",
        "plot_dataset_digits(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "rMHHnG789GPk",
        "outputId": "aad00dfb-c9b2-4be6-be45-fa0a12b90ff0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colored MNIST dataset already exists\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 936x576 with 18 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvcAAAHKCAYAAACHc2RVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debxcVZnv/+/zi8GIYJMAHSOjA9IdZ+UKOMCxhSZiI93aKlxtg0ZxvEILakRtgjigt9uhW7ttFIyiLaLoJQrCReRoOyM0KAFD0B9D6IRJkdEBXfePqujiyTl7n121d9WqtT7v1yuvrOesXXs/dZ6z66xTtfbaFkIQAAAAgMn3/407AQAAAADtYHAPAAAAZILBPQAAAJAJBvcAAABAJhjcAwAAAJlgcA8AAABkgsH9LMxs2sxePurHYjyod1mod3moeVmod1mo931lP7g3s2vM7IBx5zEbM3u0mZ1nZreYGTcdGBL1Lkvq9ZYkM/t7M9tkZreb2almdv9x5zTJqHlZqHdZUq/3pPwOz35wPwF+K+kMSSvGnQhGgnoXxMwOkrRS0jMl7SbpYZJOGGtS6BQ1Lwv1Ls5E/A4vdnBvZgvN7CtmdrOZ/aLf3tlt9nAz+0H/r/GzzGxR9Ph9zOw7ZnabmV1mZlOD5BFCWBdCOEXS2iGeDmpQ77KkUm9JyyWdEkJYG0L4haQTJR0x4L5QgZqXhXqXJZV6T8rv8GIH9+o990+o95f2rpLukfRht81LJL1M0hJJ90r6Z0kys50knS3pnZIWSTpW0plmtqM/iJnt2v9h2rWj54G5od5lSaXej5J0WRRfJmmxmW0/4PPC7Kh5Wah3WVKp90QodnAfQrg1hHBmCOHuEMIdkt4laX+32WkhhMtDCHdJerukF5jZPEkvlnROCOGcEMLvQwjnS/qhpINnOM51IYTtQgjXdfyUUIF6lyWhem8j6ZdRvLm97RBPDzOg5mWh3mVJqN4T4X7jTmBczGxrSR+QtEzSwv6XtzWzeSGE3/Xj66OHXCtpvqQd1PvL8flmdkjUP1/Shd1mjUFR77IkVO87JT0oije37xhgX6hAzctCvcuSUL0nQrHv3Es6RtKekvYOITxI0n79r1u0zS5Re1f1LqS4Rb0foNP6f91t/vfAEMJJo0gcA6HeZUml3mslPS6KHyfpxhDCrQPsC9WoeVmod1lSqfdEKGVwP9/MFkT/7qfeR2b3SLqtf9HF8TM87sVmtrT/F+M7JH2h/xfipyUdYmYHmdm8/j6nZri4o5b1LJC0VT9eYCyjNSzqXZZk6y3pU5JW9I+znaS3SVo9yJPEfVDzslDvsiRb70n5HV7K4P4c9X4oNv9bJemDkh6g3l9135N07gyPO029k3STpAWSXi9JIYTrJR0q6ThJN6v3V+EbNcP3s39xxp0VF2fs1s9p85XX90ha1/D54b6od1mSrXcI4VxJ71Pv49/r1PuoeKZfSmiGmpeFepcl2XprQn6HWwjJrsEPAAAAoIFS3rkHAAAAssfgHgAAAMgEg3sAAAAgE0MN7s1smZmtM7OrzWxlW0khXdS8LNS7LNS7PNS8LNS7DANfUGu9u35dJelASRskXSTp8BDCFe2lh5RQ87JQ77JQ7/JQ87JQ73IMc4faJ0u6OoTwM0kys9PVW2po1h8SM2NpnoSFEKxmk0Y1p97JuyWEsGNFP+d4ZmrOceqdn1bPceqdPF7TCzPba/ow03J20n1v9buh/zXki5rn5dqafupdFuqdH87xslBvSBrunfs5MbMjJR3Z9XGQBupdHmpeFupdFupdHmo++YYZ3N8gaZco3rn/tfsIIZws6WSJj3cyUFtz6p0VzvGyUO/y8JpeFs7xQgwzLeciSXuY2UPNbCtJh0la005aSBQ1Lwv1Lgv1Lg81Lwv1LsTA79yHEO41s9dJOk/SPEmnhhDWtpYZkkPNy0K9y0K9y0PNy0K9yzHwUpgDHYyPd5I2h9VyGqHeybs4hLBXmzuk5mnjHC9Oq+c49U4er+mF6WK1HAAAAAAJYXAPAAAAZILBPQAAAJAJBvcAAABAJhjcAwAAAJlgcA8AAABkgsE9AAAAkAkG9wAAAEAmBr5DLQAAxTrExWtcfFXU/ivXt779dNCyH7p4Dxc/0cU/7TAXtGaxi5dHbX9KP7VmX/Hdo/ydvv7RxW+q2VfbeOceAAAAyASDewAAACATDO4BAACATDDnHgA2W+piP1c69mwXn+3i77r4PwfKCKnYx8WfcPHvXfyIqH2O6/u8i0+M2vc0zAvtOSxqP971mYu3dzFz7pPxwqj9WNf3KhdvV7EfP4++Sf/RLo7n8/95zX7bwDv3AAAAQCYY3AMAAACZYFoOgLJ9LGq/0PVt02A/T3exn15xd9R+tev7QoPjYDwe5OKFDR77MBf7KT5bR22m5YxPPNXOT8PxnuHiH7ScC+bs3S4+JmoPM8j1K9auc/GSqP0k1zfPxU1eLtrAO/cAAABAJhjcAwAAAJlgcA8AAABkgjn3Dcx3sV8GKZ5j9QjX56fyPsrFz604rr8L9n5Rm+mZ3Xm9i69x8Tej9t+5Pj9d889c/MqK4/6Fi79RsS3mwJ+473fxiqj9365v2sUfj9o3ub7jXPzIivgU1+cnaH5OSEG8jt4nh9jP71zsl9G8dYh9Y3D3d/GeFdt+zcUfaTkXzNkuLj7GxVUD27tdHL9sX+r6rnPxtS6OX9L/r+vzOY4a79wDAAAAmWBwDwAAAGSCwT0AAACQiSLm3Md/wTzQ9S1ysV9+OvY8F9/o4n2bJOX4O5fHnujiB0Rt5tw3s6OLn+Xit0Ztf93EXS6+JWrv6vr8nHt/fUbVbau/5OL4Luh+DiDmwE/IfG3Ftpe72C+gfGXU/qXre46L/Q9FfGv7E1yff+Fhzv1o+PsYvMfF8brnfzrEcX7r4tOG2Bfa87cu3itq+xfpd7n4zvbTwdxs6+Kfuzg+Vf0Y6VgX//sQeUxF7bo59j7HrvHOPQAAAJAJBvcAAABAJrKcluP/YnlF1P7XFo/j7yjeFb8Cl1/KCXP3MRf/VYPHbu1iP+uiLX7py1tm3Ar3sSBqP9b1HVnz2Hh6jZ+L9WkX/zRq+4/l73Wxn2rzvqjt10L1a+PG9zK/WGhTPBfv5a7vr0eZCMbuRS6O51N+0/X9V8e5YM6ucPFfuvgpUXuD6zt7iOPu5+L3zbhVj59q/c4hjjsI3rkHAAAAMlE7uDezU83sJjO7PPraIjM738zW9/9f2G2aGCVqXhbqXRbqXR5qXhbqjbm8c79a0jL3tZWSLggh7CHpgn6MfKwWNS/JalHvkqwW9S7NalHzkqwW9S5a7Zz7EMI3zWx39+VD9cdVgD6p3k3a39xiXkPZysUHR21/x/hhVjfzSxv9OmovcH1N/kT+jov/xsW/arCvQUxizWezv4ufPsS+/Jy5eP7dF13fh4Y4zgdc3PU1FlnUe++ofWHNtp93cVwsf/J58UUaS13fW138BRf7dVdj27t496jd8pz7LOrdhF//eEXUHtUce/8L4b0u7vg7XVzN56pqjWJ//t7RcS4tKq3eP66J23K0i/2SnLF1Lv6PlnOpM+ic+8UhhI399iZJi1vKB+mi5mWh3mWh3uWh5mWh3gUZerWcEEIws1nvyWNmR6p+vQpMkKqaU+/8cI6XhXqXh9f0snCO52/Qd+5vNLMlktT/3892+YMQwskhhL1CCHvNtg0mwpxqTr2zwTleFupdHl7Ty8I5XpBB37lfI2m5pJP6/5/VWkYt8HPSD43afo6UX8b6MS7eFLVPcX3nuviGqO3X0/fLWlf5nosTWdc+6ZrH4nn2FzR8bLzG/DOHyKHJnHs/j88vrzwmE1NvSdKJUfsXru8kF//vIY7zlVnaUvWix955Ln6Vi98Rtc9ssN/BTVa9m/AXy/iLmGL+Z+ewqH2a6xvmgi0/B3888q35bPzNSvx9LfJWXr1n4E//+H41h7i+p7l4fsV+b3CxH1uO2lyWwvyspO9K2tPMNpjZCvV+OA40s/WSDtCWvz4xwah5Wah3Wah3eah5Wag35rJazuGzdA3zxiYSRs3LQr3LQr3LQ83LQr3BHWoBAACATAy9Ws6k8UvVHjrjVoOJl1Pet+Fjr4nafp1zNFM1pda7xsXPHfCYfsnzOndF7WsHPGbRnu3ip0ZtPzd6mDn2XTlg3AlkzF845SfSVjnDxV+L2r9We/Zx8YOj9iahK493cdWce39dDCbSfi7+jIvv39JxXubir8241ejwzj0AAACQCQb3AAAAQCaKm5bTpfjT/8c2fOyyqP3fLeRSkq1cvF2Dx37Yxb9s8Nidovabara9y8VvjNpfbnBM9L3dxRa114wykQHNG3cCGXmUi/10iqr7cPopPD+t2PYlLn6Dix/u4qq5en718B2jNtNyuvOkmv74l+/GWbdC4pZEbb8SblvTcCTp36L2d1rcbxt45x4AAADIBIN7AAAAIBMM7gEAAIBMMOd+CH41u79t8Nhvu5jpfYPbzcUvrtj2Chd/aYjjnhy1D6rZ9psVj8UAHuTiH0ftr48ykY58cNwJJC7+zfUa1+fn2P/OxR+P2n6OfdVyl/4k9vGxLn5vxb7udvG9FduiPS9ysbn43Kh9Z8e5oDPxiqdP7PA4t0dtf0qPG+/cAwAAAJlgcA8AAABkgsE9AAAAkAnm3Dfg59j/h4u3r3jsz118oouZ3je49S7+SNR+nev7Gxdf2+A4z3Lxshm36vF/NR/d4DgYQDzh8baxZVFth6jtF1v+sYuHuRikBEdF7VfVbPvvLv5fLecyiFNdfOVYsihDfA+BXV1fcPGFHeeCkfhq1D7O9flLY+Jx3FrXt7rmOP8zavtT+uqax3aNd+4BAACATDC4BwAAADLBtJwa20btd7i+qmk43mkuPn+wdDAH8ZSnf3Z91w2xX383ef+Jbmxli8fFhPLLdcZTbR7i+i528S3tpzPRnutiP6+xylltJhKZ7+I9Krb1067WtZwLZvfaqO2XSfXu6DIRjIMfA/g45mdLrq7Z905Re+FcExoR3rkHAAAAMsHgHgAAAMgEg3sAAAAgE8y5rxFPo9y7weP8Mpn/0EIumJtbZmk3daiLj2/w2Ko7zyNT/tX0DS5+atT2c+xf2X46WfHfSz85NvZ1F/v57oPyc+z9hTUvd3E8f/vdru+MVjLCXBxe0Xeui9d0mQgeH7X3dH2fG2UiLYkvo7pobFnMjHfuAQAAgEwwuAcAAAAyweAeAAAAyARz7p2Hu/gTDR4bz7N/teu7c7B0MEZHu3jrim2/0GUiqBcvOPxnru8nHR3Tr2N/nourLtJ5oYs3DZ9OVg5w8ZMrtp128SEu/tUQeewbtY9wfX6OvRfnfNUQOaCZHVy87Yxb9ZzZZSLwv0PfFbX94PNkF384ar+1tYy2tE3UzulSGN65BwAAADLB4B4AAADIBIN7AAAAIBPMuXce4OJHV2x7iYvjefbMsZ8M27k4Xrd2yvX93sWnRO0j20oIg9k5ar/M9b2pxePE83lf4fr8HHv/AvEvUfuathLKSHythF/0el7F485xcZM59vu6+O9dPBW1t6/Z1ykuvqZBHmiPv3DuwRXb/neXiWArFy+I2htdn7+NxLFR21+i1Kb4UqmDarb9mYub3Ptm1GrfuTezXczsQjO7wszWmtlR/a8vMrPzzWx9//+F3aeLrlHv8lDzslDvslDv8lBzzGVazr2SjgkhLJW0j6TXmtlS9e7Pd0EIYQ9JF2jL+/VhMlHv8lDzslDvslDv8lDzwtVOywkhbFT/E5QQwh1mdqV6C88dqj9+aPlJ9RYke3MnWXboYBd/sMFjv+/iHKbi5F5vbz8XPz1q+2k4N7n439tPZywmsuY/dnE8pePvXN9HXew/W63il2SM12Sbcn2XuvgfXXx6g+N2KNl63z9q+/lyVQ5z8eMaPPZ5Ll4w41Y9/gXhX1z8aRf/pkEeHUq23l3Zy8Uhav/C9V3QcS5jMgk1/1cXX+3i+PR6aId5vKrBtn65zivbTKRljS6oNbPdJT1BvXHt4v4PkNRbpXlxq5lh7Kh3eah5Wah3Wah3eah5meZ8Qa2ZbaPeLR+ODiHcbmZ/6AshBDMLszzuSHG94cSh3uWh5mWh3mWh3uWh5uWa0zv3ZjZfvR+Qz4QQvtj/8o1mtqTfv0RbzlqQJIUQTg4h7BVC8B+WIVHUuzzUvCzUuyzUuzzUvGy179xb70+9UyRdGUJ4f9S1RtJySSf1/z+rkwxbto2L3+Jiv4pW7DQXt7nCXipyq7f3QBf7le9it7n4CBdfPHQ2aZjImn/IxfEaZv6D5gNdXHWxxKtd7OfNx/Ou17i+V7p4U8Vxxmgi613liTXxMC6K2p9yfX7ScKKyq3edYyv6zMW7V2x7s4v9L4SETULNT3Txd11876gSifiPMT7s4g+MKpEWzGVazlPVu0Ttx2a2+ZKx49T74TjDzFZIulbSC7pJESNGvctDzctCvctCvctDzQs3l9VyvqUt/97d7JntpoNxo97loeZlod5lod7loeZotFoOAAAAgHTNebWcSbZ11P6s63tKzWNvjdr/5PruHjgjjIu/j8HTZ9yqx19jcd6MW2EsvuPi86O2X7v8tS4+pGK//j0tf3v6d0XtUyv2g8lwiYv9C8TXovaNHeeCdvgL62L+/gk/cXH8XvfrXZ+fgI1afowUz2n3Hyvs23Eus7kjavvx4dGjTKRlvHMPAAAAZILBPQAAAJCJLKfl+DuIfz5qL6t57M9d/DdR29/xHunzd6J/jotnu+JIkr7Vci7o0Gui9o6ubz8XP7piP/529C90sX+BQHs2RG1/+5yXuDhe7vRS13e+5u4/XHxPg8ciTX752ni9621d34Uu/mHU/nJrGRXLz2SKp+U8wfW9tMXj/lfUPr1m27Oj9pUt5jBuvHMPAAAAZILBPQAAAJAJBvcAAABAJiwEf8PdDg9mNpKDHeziJlPn1rr4sUPmMklCCFVT0BsbVb29pVH7665vBxff5eJ46atPtJZRsi4OIezV5g7HVfP78MvdfdXFe7v4uKj9Edd3h7KSyzmOOWv1HKfeycvzNR2zmu01nXfuAQAAgEwwuAcAAAAyweAeAAAAyESW69wf2GDbV7rYL3uMyRMvke3n2Hv/7eIC5tnn7zYXj+u+5gAAjAHv3AMAAACZYHAPAAAAZILBPQAAAJCJLOfcv8fFr6/Y9n+4+OMt54K0XOHi940lCwAAgG7wzj0AAACQCQb3AAAAQCaynJZzk4vnjSULjMvRs7QBAAByxzv3AAAAQCYY3AMAAACZYHAPAAAAZGLUc+5vkXStpB367ZSUntNuHeyTejcz6py6qvld4ns7F7nUm3N87ia95tS7mUmvt8RrehPJ1NtCCCPMo39Qsx+GEPYa+YErkFN3Unwe5NSdFJ8HOXUrxedCTt1J8XmQU3dSfB7kVI1pOQAAAEAmGNwDAAAAmRjX4P7kMR23Cjl1J8XnQU7dSfF5kFO3Unwu5NSdFJ8HOXUnxedBThXGMuceAAAAQPuYlgMAAABkYqSDezNbZmbrzOxqM1s5ymNHOZxqZjeZ2eXR1xaZ2flmtr7//8IR57SLmV1oZleY2VozOyqFvNpAzWfMh3p3m0NS9e4fP8uap1Dvfh5J1TzXektp1Dy1evePn2XNqfesOSVd75EN7s1snqSPSHqWpKWSDjezpaM6fmS1pGXuayslXRBC2EPSBf14lO6VdEwIYamkfSS9tv+9GXdeQ6Hms6Le3VqttOotZVjzhOotpVfz7OotJVXz1Uqr3lKGNafeldKudwhhJP8k7SvpvCh+i6S3jOr4LpfdJV0exeskLem3l0haN468onzOknRganlRc+pNval5ivVOveY51Du1mqdc71xqTr0nt96jnJazk6Tro3hD/2spWBxC2Nhvb5K0eFyJmNnukp4g6ftKKK8BUfMa1HtkkvneZlTzlOstJfK9zajeUto1T+Z7m1HNqfccpFhvLqh1Qu/PrbEsIWRm20g6U9LRIYTbU8krd+P63lLv8eAcLw/neFk4x8tCvbc0ysH9DZJ2ieKd+19LwY1mtkSS+v/fNOoEzGy+ej8gnwkhfDGVvIZEzWdBvUdu7N/bDGuecr0lzvEupFzzsX9vM6w59a6Qcr1HObi/SNIeZvZQM9tK0mGS1ozw+FXWSFreby9Xb+7UyJiZSTpF0pUhhPenklcLqPkMqPdYcI63L+V6S5zjXUi55pzj7aPes0i+3iO+4OBgSVdJ+qmkt47jIgNJn5W0UdJv1Zs/tkLS9upd1bxe0tckLRpxTk9T76ObH0m6tP/v4HHnRc2pN/Wm5qnXO8Wa51rvVGqeWr1zrjn1nsx6c4daAAAAIBNcUAsAAABkgsE9AAAAkAkG9wAAAEAmGNwDAAAAmWBwDwAAAGSCwT0AAACQCQb3AAAAQCYY3AMAAACZYHAPAAAAZILBPQAAAJAJBvcAAABAJhjcAwAAAJlgcA8AAABkgsE9AAAAkAkG9wAAAEAmGNwDAAAAmWBwDwAAAGSCwT0AAACQCQb3szCzaTN7+agfi/Gg3mWh3uWh5mWh3mWh3veV/eDezK4xswPGnUcVM/t7M9tkZreb2almdv9x5zSpUq+3mT3azM4zs1vMLIw7n0lHvctDzcuSer0lfoe3KfV6T8r5nf3gPnVmdpCklZKeKWk3SQ+TdMJYk0KXfivpDEkrxp0IRoJ6l4eaF4Tf4cWZiPO72MG9mS00s6+Y2c1m9ot+e2e32cPN7Af9v8bPMrNF0eP3MbPvmNltZnaZmU0NmMpySaeEENaGEH4h6URJRwy4L8wilXqHENaFEE6RtHaIp4Ma1Ls81LwsqdRb/A4fiVTqPSnnd7GDe/We+yfU+0t7V0n3SPqw2+Ylkl4maYmkeyX9sySZ2U6Szpb0TkmLJB0r6Uwz29EfxMx27f8w7TpLHo+SdFkUXyZpsZltP+DzwsxSqTdGg3qXh5qXJZV68zt8NFKp90QodnAfQrg1hHBmCOHuEMIdkt4laX+32WkhhMtDCHdJerukF5jZPEkvlnROCOGcEMLvQwjnS/qhpINnOM51IYTtQgjXzZLKNpJ+GcWb29sO8fTgJFRvjAD1Lg81L0tC9eZ3+AgkVO+JcL9xJzAuZra1pA9IWiZpYf/L25rZvBDC7/rx9dFDrpU0X9IO6v3l+HwzOyTqny/pwgFSuVPSg6J4c/uOAfaFWSRUb4wA9S4PNS9LQvXmd/gIJFTviVDs4F7SMZL2lLR3CGGTmT1e0n9JsmibXaL2rupdSHGLej9Ap4UQXtFCHmslPU69CzTUb98YQri1hX3jj1KpN0aDepeHmpcllXrzO3w0Uqn3RChlWs58M1sQ/bufeh+Z3SPptv5FF8fP8LgXm9nS/l+M75D0hf5fiJ+WdIiZHWRm8/r7nJrh4o65+JSkFf3jbCfpbZJWD/Ik8QfJ1tt6Fkjaqh8vMJZNGxb1Lg81L0uy9Ra/w7uQbL0n5fwuZXB/jno/FJv/rZL0QUkPUO+vuu9JOneGx52m3km6SdICSa+XpBDC9ZIOlXScpJvV+6vwjZrh+2m9izPutFkuzgghnCvpfep9PHSdeh8lzfRDi7lLtt7qfTx4j/54pf09ktY1fH64L+pdHmpelmTrze/wTiRbb03I+W0hJLsGPwAAAIAGSnnnHgAAAMgeg3sAAAAgEwzuAQAAgEwMNbg3s2Vmts7MrjazlW0lhXRR87JQ77JQ7/JQ87JQ7zIMfEGt9e76dZWkAyVtkHSRpMNDCFe0lx5SQs3LQr3LQr3LQ83LQr3LMcxNrJ4s6eoQws8kycxOV2+poVl/SMyMpXkSFkKwmk0a1Zx6J++WEMKOFf2c45mpOcepd35aPcepd/J4TS/MbK/pw0zL2Un3vdXvhv7XkC9qnpdra/qpd1mod344x8tCvSFpuHfu58TMjpR0ZNfHQRqod3moeVmod1mod3mo+eQbZnB/g6Rdonjn/tfuI4RwsqSTJT7eyUBtzal3VjjHy0K9nfjJfc71HTbKRLrDa3pZOMcLMcy0nIsk7WFmDzWzrdR7rVvTTlpIFDUvC/UuC/UuDzUvC/UuxMDv3IcQ7jWz10k6T9I8SaeGENa2lhmSQ83LQr3LQr3LQ83LQr3LMfBSmAMdjI93kjaH1XIaod7JuziEsFebO6TmaeMcr/YPLj4+ap/h+g7vOJeWtHqO51bvDPGaXpguVssBAAAAkBAG9wAAAEAmGNwDAAAAmeh8nXsAAFK02MWvqNi27u5AyMDTovbfuL43uPhkF78+av+6tYyAgfDOPQAAAJAJBvcAAABAJhjcAwAAAJlgzj3Qgke7+DwXb+3ihR3mAmB28Tz717m+JS4+O2qf0E06GCc/j35l1F7k+n7v4hUuvjFq+xsmACPGO/cAAABAJhjcAwAAAJlgcA8AAABkgjn3QAv8HPsHu/guF+8eta9pOxkMbKombmL/ir5vuHjVEMdBtce7+MtR28+x9/4pat/TTjoYp5e5eKWL/Tz72HoXP8LFlwyUESbIlIuPr+h7hounW86lDu/cAwAAAJlgcA8AAABkorxpOa908b81eKy5OFRs+68uPs7Ftzc4LpLnP973PxoPdHF8m/u3tp8OGlgVtY+fbaO+6ajtp9Z4df0YDb9i4UMqtvU1o4YZeFLU/pjr8y/UV0Xtd1bsR5Je4OL/apgXxmKqIl7l+nxc9/shdqGL42k60w32MyjeuQcAAAAyweAeAAAAyASDewAAACATZcy5j+fZv8b1Vc2br1P12Fe7+NkuPtTFPxoiDwBz5udCTkXtadd3got9P9LzSBe/0MXxy/ZXXd/h7aeDUfPLXcbz7P11c5e7eFnU3uj6Pu1iP+f+2vrUMBrxa/xUg8c1mVPf1FTUnu7wOJvxzj0AAACQCQb3AAAAQCYY3AMAAACZyHPO/Xtc/OaoPcwcez+nbteK/t1qtn2vi58fte9smBfG4jENtr3XxVfNuBVGYcrF01Hb3zIck+dcFy9y8R1R+/2uj5feCbS/i09ycfw7/zeu72gX+3n2sU+6+Hs1eWFkqq6jKhXv3AMAAACZYHAPAAAAZCKPaTl7uPjvK7b9nYsvcPGXXHxm1PtogZgAACAASURBVP6167u/i+P+/9/1+c+GD3Lx56K2XzYTSXiii8+ccauZXe9i/wkvgLmLf3G90vX5GZF3uTj+9eA/zscE2NrF73ax/10b+18ubvID4F/wNzR4LFo1zOxqvxrq1CxtacslK/1SmX77KqsabNsG3rkHAAAAMsHgHgAAAMhE7eDezE41s5vM7PLoa4vM7HwzW9//f2G3aWKUqHlZqHdZqHd5qHlZqDcshOrZS2a2n3orhH0qhPDo/tfeJ+nnIYSTzGylpIUhhDdX7af/uGGmSs3uoy5+hT9w1P6263t6i3nE+zrf9c13sZ/4dXPUXtxaRo2EEExqr+ad1XtMvunip0ZtX05/ycUhLr6ilYyGdnEIYa+JOMdbtMrF8TxKvxTmdKeZjF4IwXKo92uj9odcnz8Xz3Dx4e2nk7JWz/Ekzu8vu/hZNdtfErWf3HIu6cnyNX2Vi/3cd286are5vHGTPE6oeWxbNo/bvNp37kMI35T0c/flQ/XHawI/Kemvh8oOSaHmZaHeZaHe5aHmZaHeGHTO/eIQwubbPWzS2N5rxghR87JQ77JQ7/JQ87JQ74IMvRRm6H3OO+vHNmZ2pKQjhz0O0lFVc+qdH87xslDv8vCaXhbO8fwNOri/0cyWhBA2mtkSSTfNtmEI4WRJJ0stz906NGrX/QjGM5Je3loGW4onVm9Vs62fJeWvBUjPnGreWb0T58vpl0BOZI59E+M/xzuyysXHz9KW8ptzX2Gi6r1/1Pbnnv84+j87yuEhLn5hxbaXuPgbLecyoMl5Td82avv7wPiMLnPxM9tPR5K0fUXfrR0dczgTdY7H6ubYe36++6D8bRCmKraddvGqlnIY1KDTctZIWt5vL5d0VjvpIGHUvCzUuyzUuzzUvCzUuyBzWQrzs5K+K2lPM9tgZisknSTpQDNbL+mAfoxMUPOyUO+yUO/yUPOyUG/UTssJIcy2clhXH3ZhzKh5Wah3Wah3eah5Wag3hr6gNgl1M8J+GrX94lDDWOLiV0btprPU1g2ZC1p3hIsfU7GtL/f7200FHYrnbPs6+jmXba6ZjLl7mYvjpc19zX7i4tNbyuGvXPwxF+9Y8di7XOzn3H8wan+9SVKl+FLU9gW/w8Un1vQP6i9d7H8A4ry+7/q+VvNYDMXPsZ8ecD9N5th7qf1uGHTOPQAAAIDEMLgHAAAAMjG503L2aLDte6L2zS3msNrF27S4b4xFfMu+f3N9VaubftPFXS2/h275j3f9EmxTUXu600zK5lcZPMrFD6h47DIXDzMTc9+o/TbX56fhXOXieDXEp7i+g128MGozLUdbzgz338DY2138pRm3au4fXOxfDKqm3u7s4ue4OJ4qTMFr+SkvfvqML82qlo7j9zNdE6eEd+4BAACATDC4BwAAADLB4B4AAADIxOTOuf8/UdtPuNraxW+J2n5y9NUNjrmfi5/Y4LGYCHtH7QWur2qK5VT7qWAMVrl4fxfHcz1N6MpfuPhRFdt+0sXXNjjOw138bhf/bdT25/8PXPwCF2+I2r+ryeORUXsf1/e9msdmyc+5jy94utP1+d/pw4jn2b/V9fnj/rWL10dtPzbw1wG8KWoz577WVMPt49fpYZbJXNXwuCnhnXsAAAAgEwzuAQAAgEwwuAcAAAAyMblz7uO58pe5vn1dHE+s/Kjru3+DYz7VxVWTsDERlrg4vs39712fv4X8m9tPB4nx8zWnovYq1+djdOe2qP2RIfbj59g/r2LbS2u23ThEHndE7Vtn3Spju7t4uYvjC1zWuD7/+7+JlS6Or9872/X5teqr/NLF/gKdjzfYF2pfW6vuRzLl+qZdfEJF3yTjnXsAAAAgEwzuAQAAgExM7rSc2NNcfKaLnxu1/RprTdStffefUfvpDfe1uHk6aO4JLvZ3GN+h4rFrXfxvw6eDxE1X9PllMtEe//Lo43iK3CUN9/2QqP1I1+ePE7/7darr89Nw/srFx8yyH0n6iYuXRe0mS3lmw8+P9L8P4ymwfrrMMPwvhO9G7RcOsd+Xu5gpvK2adrGfllNlysXfqNjvJOOdewAAACATDO4BAACATDC4BwAAADKRx5x772UujufvPdn1zXOxv0/496P2g13fZ1x8edR+iuur+zPqVzX9aMVOLm6yutmxbSbSwFTU/q7r+/UI84D0jKh9oeubcvF0p5nkzU9R9vGCqO3nzV9Vs+/4nH9MzXHi5XD9r4Y3uthfv/OAWfYjbbmsZpHz7Ad1eof73hC172n42F2i9ktdn9/XBqGBKRc3mWNfJ9drp3jnHgAAAMgEg3sAAAAgEwzuAQAAgEzkOefe3/o5Xgffr3N/qIt/5uIPRe1dXd91FTn8q4u3r9hW2jJndOLR405gALtH7R+MKwlIqp5HP9VgWwwnfjn9qut7sYsf5+KTBjzmRwZ83EzHXD3Evor3JBdfPJYstryxwbuj9lLX58cZ32s/nZxN1cTTLo6vjVrl+vx8/XhfflsfTxLeuQcAAAAyweAeAAAAyASDewAAACATec65r/L1mrhK1Rx7JOkFLn57g8fe5eJxrSm/ekzHLUW8trkNsZ/pIfPAH/n7OXzbxU+I2ru5vm+52K9d35X3uzi+PuYLI8phYvmC+4uL9o7a/gYTB7vY/wA0sXXU3sX1+V8eK1wcr2X/Zdd39hA5FWpV1K5b1/6EOe5H2nJd+6mK40zXxCnjnXsAAAAgE7WDezPbxcwuNLMrzGytmR3V//oiMzvfzNb3/1/YfbroGvUuDzUvC/UuC/UuDzWHhVD9oaWZLZG0JIRwiZltq97CU38t6QhJPw8hnGRmKyUtDCG8uWZfo/qEdDziz4d/5Pq2cbH//H9t1Pb3RB+REILlUO/41vR+mbz9ah57btT+qOvzn7Rm4OIQwl451HwYccLPcH3Tc3ycNNyUnlHJ5Rx/dtR+g+ubcvEwCcY19fv5kIv968X6IY7booslHaJJq/dxLn5HnITru9XFftpOlX1cvFPFtv6461z81qj9pQY5tCub1/S4jFOub9rFflqO759tvzPtO9bk98G4hBBm/NVT+859CGFjCOGSfvsOSVeqdwocKumT/c0+qd4PDiYc9S4PNS8L9S4L9S4PNUejOfdmtrt61zJ9X9LiEMLGftcmSYtbzQxjR73LQ83LQr3LQr3LQ83LNOfVcsxsG0lnSjo6hHC72R8/CQi9z3pn/OjGzI6UdOSwiWK0qHd5qHlZqHdZqHd5qHm5aufcS5KZzZf0FUnnhRDe3//aOklTIYSN/fld0yGEPWv2M3HzcQd2s4u3d7GfJRVv/6ftpzMXm+duTXq9t4vafjqm55e7fFHUznCOvXdxCGEvafJrPoxVDfri2C+bNilz7qWy612YzXOwJ6ve/vfle6P2S11fk4z8SVr12Ktc/DIX+4sq6n7ZjEY2r+nxQaddn58L701Fbf86PaXZ+bn7q2qOk4KB59xb70+9UyRdufkHpG+NpOX99nJJZw2bJMaPepeHmpeFepeFepeHmmMu03KeKunvJP3YzC7tf+04SSdJOsPMVki6VlveLwiTiXqXh5qXhXqXhXqXh5oXrnZwH0L4lmb/xPmZ7aaDcaPe5aHmZaHeZaHe5aHmmPMFtWjIz1Krm7XGzNXWNPlW+tsRFDDPHs6qqN3kZ6fqlucAhuDnr788aq9yfc928cEuXhK1v+n6/An/uajt59T/UujQVIO+VTX78vPsq8Sv43X7nSSNlsIEAAAAkC4G9wAAAEAmmJbTletc7Jf28raK2g92fZuGT6ckVUsSXu7iF824FUrlp9pUfbzLtBxgDDa4+N9rYkyEqQbbNpl2403CksVt4J17AAAAIBMM7gEAAIBMMLgHAAAAMsGc+67s7+L3uPh1Lp4XtbdpP52S3Ba15826FbCl6Zr++LSu2xYAMDerXBy/1k7VPHbaxSdU9JWCd+4BAACATDC4BwAAADLB4B4AAADIBHPuu3KXiy918Y0uPjFqX91+OgDqTdfEAIDuPWPcCUw43rkHAAAAMsHgHgAAAMgEg3sAAAAgE8y5H5VTa2IAAABgSLxzDwAAAGSCwT0AAACQCQb3AAAAQCYY3AMAAACZYHAPAAAAZILBPQAAAJCJUS+FeYukayXt0G+npPScdutgn9S7mVHn1FXN7xLf27nIpd6c43M36TWn3s1Mer0lXtObSKbeFkIYYR79g5r9MISw18gPXIGcupPi8yCn7qT4PMipWyk+F3LqTorPg5y6k+LzIKdqTMsBAAAAMsHgHgAAAMjEuAb3J4/puFXIqTspPg9y6k6Kz4OcupXicyGn7qT4PMipOyk+D3KqMJY59wAAAADax7QcAAAAIBMjHdyb2TIzW2dmV5vZylEeO8rhVDO7ycwuj762yMzON7P1/f8XjjinXczsQjO7wszWmtlRKeTVBmo+Yz7Uu9sckqp3//hZ1jyFevfzSKrmudZbSqPmqdW7f/wsa069Z80p6XqPbHBvZvMkfUTSsyQtlXS4mS0d1fEjqyUtc19bKemCEMIeki7ox6N0r6RjQghLJe0j6bX978248xoKNZ8V9e7WaqVVbynDmidUbym9mmdXbympmq9WWvWWMqw59a6Udr1DCCP5J2lfSedF8VskvWVUx3e57C7p8iheJ2lJv71E0rpx5BXlc5akA1PLi5pTb+pNzVOsd+o1z6HeqdU85XrnUnPqPbn1HuW0nJ0kXR/FG/pfS8HiEMLGfnuTpMXjSsTMdpf0BEnfV0J5DYia16DeI5PM9zajmqdcbymR721G9ZbSrnky39uMak695yDFenNBrRN6f26NZQkhM9tG0pmSjg4h3J5KXrkb1/eWeo8H53h5OMfLwjleFuq9pVEO7m+QtEsU79z/WgpuNLMlktT//6ZRJ2Bm89X7AflMCOGLqeQ1JGo+C+o9cmP/3mZY85TrLXGOdyHlmo/9e5thzal3hZTrPcrB/UWS9jCzh5rZVpIOk7RmhMevskbS8n57uXpzp0bGzEzSKZKuDCG8P5W8WkDNZ0C9x4JzvH0p11viHO9CyjXnHG8f9Z5F8vUe8QUHB0u6StJPJb11HBcZSPqspI2Sfqve/LEVkrZX76rm9ZK+JmnRiHN6mnof3fxI0qX9fwePOy9qTr2pNzVPvd4p1jzXeqdS89TqnXPNqfdk1ps71AIAAACZ4IJaAAAAIBMM7gEAAIBMMLgHAAAAMsHgHgAAAMgEg3sAAAAgEwzuAQAAgEwwuAcAAAAyweAeAAAAyASDewAAACATDO4BAACATDC4BwAAADLB4B4AAADIBIN7AAAAIBMM7gEAAIBMMLgHAAAAMsHgHgAAAMgEg3sAAAAgEwzuAQAAgEwwuJ+FmU2b2ctH/ViMB/UuC/UuDzUvC/UuC/W+r+wH92Z2jZkdMO48ZmNmjzaz88zsFjML485n0qVeb0kys783s01mdruZnWpm9x93TpOKepeHmpcl9XrzO7xd1Lsd2Q/uJ8BvJZ0hacW4E0H3zOwgSSslPVPSbpIeJumEsSaFzlDv8lDz4vA7vCwTUe9iB/dmttDMvmJmN5vZL/rtnd1mDzezH/TffTnLzBZFj9/HzL5jZreZ2WVmNjVIHiGEdSGEUyStHeLpoEYq9Za0XNIpIYS1IYRfSDpR0hED7guzoN7loeZlSaXe/A4fDerdTLGDe/We+yfUe2dlV0n3SPqw2+Ylkl4maYmkeyX9sySZ2U6Szpb0TkmLJB0r6Uwz29EfxMx27f8w7drR88DcpFLvR0m6LIovk7TYzLYf8HlhZtS7PNS8LKnUG6NBvRsodnAfQrg1hHBmCOHuEMIdkt4laX+32WkhhMtDCHdJerukF5jZPEkvlnROCOGcEMLvQwjnS/qhpINnOM51IYTtQgjXdfyUUCGhem8j6ZdRvLm97RBPDw71Lg81L0tC9cYIUO9m7jfuBMbFzLaW9AFJyyQt7H95WzObF0L4XT++PnrItZLmS9pBvb8cn29mh0T98yVd2G3WGFRC9b5T0oOieHP7jgH2hVlQ7/JQ87IkVG+MAPVupth37iUdI2lPSXuHEB4kab/+1y3aZpeovat6F1Lcot4P0Gn9v+42/3tgCOGkUSSOgaRS77WSHhfFj5N0Ywjh1gH2hdlR7/JQ87KkUm+MBvVuoJTB/XwzWxD9u596H5HeI+m2/kUXx8/wuBeb2dL+X4zvkPSF/l+In5Z0iJkdZGbz+vucmuHijlrWs0DSVv14gbFs2rCSrbekT0la0T/OdpLeJmn1IE8Sf0C9y0PNy5Jsvfkd3gnqPaRSBvfnqPdDsfnfKkkflPQA9f6q+56kc2d43GnqvShvkrRA0uslKYRwvaRDJR0n6Wb1/ip8o2b4fvYvzriz4uKM3fo5bb7y+h5J6xo+P9xXsvUOIZwr6X3qfRx4nXofHc70IoW5o97loeZlSbbe4nd4F6j3kCyEZNfgBwAAANBAKe/cAwAAANljcA8AAABkgsE9AAAAkImhBvdmtszM1pnZ1Wa2sq2kkC5qXhbqXRbqXR5qXhbqXYaBL6i13l2/rpJ0oKQNki6SdHgI4Yr20kNKqHlZqHdZqHd5qHlZqHc5hrlD7ZMlXR1C+Jkkmdnp6i01NOsPiZmxNE/CQghWs0mjmlPv5N0SQtixop9zPDM15zj1zk+r5zj1Th6v6YWZ7TV9mGk5O+m+t/rd0P8a8kXN83JtTT/1Lgv1zg/neFmoNyQN9879nJjZkZKO7Po4SAP1Lg81Lwv1Lgv1Lg81n3zDDO5vkLRLFO/c/9p9hBBOlnSyxMc7GaitOfXOCud4Wah3eXhNLwvneCGGmZZzkaQ9zOyhZraVpMMkrWknLSSKmpeFepeFepeHmpeFehdi4HfuQwj3mtnrJJ0naZ6kU0MIa1vLDMmh5mWh3mWh3uWh5mWh3uUYeCnMgQ7GxztJm8NqOY1Q7+RdHELYq80dUvO0cY4Xp9VznHonj9f0wnSxWg4AAACAhDC4BwAAADLB4B4AAADIBIN7AAAAIBOd38SqJFNR+0LX1+pVbAAAAMAMeOceAAAAyASDewAAACATDO4BAACATDDnvkVTFX2ramIAHdnaxdu7eJuo/fKafT0xal/i+ja6+F9c/OuafSNtS13s7+v5HRc/tcNcAKAC79wDAAAAmWBwDwAAAGSCwT0AAACQCebcO6tcvH/UfsYI88CIfMrFL4rafs7s9zrOBe14pIs/4eK9XRzfhCI0OM5+LvaPfaiL3xC1mX8/+X7v4j1d/Pmovdz13d1+OqXwL8uPcfFeDfb1zy7+UdR+guv7totPcPF7GxwX6Brv3AMAAACZYHAPAAAAZKK4aTlTLj6+pt9/9IYJN9/F27k4nlrxPNfHtJzJsK+L/TScUXmli98TtTeMMhGMxEIXXxi1mYZT6f4ufrOLD4vaf+b6msyk8w538VVR+1LX53N8xBDHBbrGO/cAAABAJhjcAwAAAJlgcA8AAABkorg593Vz7KddvKqrRDAeT3TxwRXb+mUyh/EQFx8Rtf1t66dbPG4pto3aRzV87Dei9sWu73Mu3lSxn2fXHGfjnDNCKv4kar+rZttfufj2lnPJ2DIX+9/TsZNdfM4Qx32pi+N59C8ZYr/o1rYuPsPF8eUvK1yfX8H46VH7hcMkVePQqP2VDo+zGe/cAwAAAJlgcA8AAABkgsE9AAAAkIki5tzHyw1Pub5pFz+j00wwdm+v6f9u1L66xeN+0sXxD9pNrs/Pz0e9VVH7cTXbnljx2GH8e0v7QTpeEbWfU7PtJS7+dMu5QJJ0iosvGmJfa1y8U9S+ruaxdZfYoD2LXPxRFx9Y8divutj/erWoPcw9E+q8IGoz5x4AAADAnDG4BwAAADKR5bScqYp42vW1OQ1n/xb3hY48y8X+c7g7o/Y9LR53h4q+H7Z4nFItidp1n61+vMtEMNF2dPExY8miOP/pYj/V5n9E7c+4vte4+GtD5PHqir6rXPyJIY6DLZmLHxS1P+b6DtXcDTPL9W4X+3fDFwyx767xzj0AAACQidrBvZmdamY3mdnl0dcWmdn5Zra+///Cqn1gslDzslDvslDv8lDzslBvzOWd+9Xa8gZyKyVdEELYQ9IF/Rj5WC1qXpLVot4lWS3qXZrVouYlWS3qXTQLoX7xHzPbXdJXQgiP7sfrJE2FEDaa2RJJ0yGEPeewn05WGppy8YUzbdTn59hPt5hH1ZPr8rhtCSH8YdpbGzXvqt6NHOzis118g4ufFrWvGeK4j3Sxn1S6ddT+C9c3zNpuzVwcQthLSv8cr/W7qO0z8OumvcHFv2k/nVRtPscnvt5dWeLiDQ0e+x0XP33GrUat1XN8VPXezcXnR+09XJ+/NOrNLv6XiuM818XxMpv/7fqe4uJfVux3jCbmNX0XF69w8dtaOo5fafpUF1cthfl5F/tL9t7ZII8jonabq+TG47bYoHPuF4cQNvbbmyQtHnA/mBzUvCzUuyzUuzzUvCzUuyBDr5YTQghVf9mZ2ZGSjhz2OEhHVc2pd344x8tCvcvDa3pZOMfzN+g79zf2P9ZR/3//yccfhBBODiHstfmjIkysOdWcemeDc7ws1Ls8vKaXhXO8IIO+c79G0nJJJ/X/P6u1jAYwVdN/QtSeHuFxY20ed0ySqnmleN78Ga7v9y5e7eJrBjzmk1z8Hhdv7+J/itqjm2PfRNr1fkWDba9wcUFz7Bvovt7zXfwwF69r/YjNPWqIx/qfs/Qle45f6+J4nXt/2dSTXeznQceXP33W9X3BxXdF7ee7vkTn2DeRVL39dRVN5tj7X5n+r5Rjo/Ydrm9Tg+M8wcX+UroqJ7n4Pxo8tg1zWQrzs5K+K2lPM9tgZivUy/tAM1sv6QBt+Twwwah5Wah3Wah3eah5Wag3at+5DyEcPkvXM1vOBYmg5mWh3mWh3uWh5mWh3uAOtQAAAEAmhl4tZxIcX9G3qqP9Tg+xXwzpoVF7Qc22PxniOPGEPD978cE1jz1zwGMe4OKLXfyLAfc7aT7m4vgahge6Pr/Q9Utd/M0Gx31i1J5yff56jvUujicLr3J9fmJojvxbSYvGkkW1twzxWH+dDVoTz3d/mus7ysVvcvFrZmnP5NKoPXmXUOQlrvm3Xd/LXTzrlcEN+Utu/K/1h1Q81ufwCRf7Xw9d4517AAAAIBMM7gEAAIBMZDEtZ9rF+7t4Kmr7qTQ+PsHF07O0/X49vx+M0J832PZEFx9Tse0PXRzfqr5uGs77XHxJzfax+LNA/9H/sxvsJ2dvm6UtbTn9w69vFk+1aXKjdf85q3/sI1wczx/4nevzcwly9GsXf3csWdyXn6K1X4PH+hd5v34jRuJDLt7oYr/8ZZV9orZfSsa/hP+8wX6xpWtcfJyLvx+1v9FhHnHN/bStqmk43uku/ulg6bSGd+4BAACATDC4BwAAADLB4B4AAADIhIXQZJLpkAczG93BIquidtXylcOajtrP6PA4XQkhWJv7G1m9p1x8btT2V5X4ZzhMhvG+/H78/Fu/fpufGBo72MXxdQGPc31+UmCzNcEuDiHs1egRNcZ1jt+Hv+jm1S7ew8U3RO29Xd+FFcfxS2j66y7+wsXx5M7fuj5/7cTXK447hIk9x9u0VdT+juvz12PENrj4L128buCMutTqOT4J9T7Wxe+N2le5Pr9abdUlTJe5+PUu/lZNXiOS52t6h+JfD37V5DrvnKUtSfcOlk5js72m8849AAAAkAkG9wAAAEAmGNwDAAAAmShizn2VKRf7Ofm+f678EsirBtzPKE3MfFy/fvj/dfFuUftu13e2i/3a9VXe7uIHRW2/5rmfc/+Fiv2+0cV+X7+J2h9zfX7iZzNlzs/c1sX3RG2/Jv4w9zXf2sXxhN8lru98Fy8b4rgVJuYc79IDovadDR73by5+XQu5dC/7Ofd/7eIvuvgnUXtpzb4WRO0vub6Dah771qjtb0cyQmW+pjewk4uno/ZDax7rfx3Et0jZNGhCQ2LOPQAAAJA5BvcAAABAJvxCgcWZrolXRe0my2j6bX3sl8r0x4XzJ1H7q65vVxfH9wX/G9c3zHplb3DxNlHbf3Dpc/KPjf3GxWtd/OGofWrFfjA3d1T0DTMNx/NTwuLpVv5tFT9VCN3x97mvclfU/mjbiaANz3Wxfyn+UYN9/SpqP9/1vdvFr3Xx26K2fwlf0yAHtMtPwznHxQ+L2v5nx/+6PcXF45qKMxe8cw8AAABkgsE9AAAAkAkG9wAAAEAmip9zX8ffyb5KvPxl3fx8f1f7+LGrGhyzGPGfoVe7Ph/H9xsfZo79Q1y81Yxb9fiCvs/FVYuTfcfF36hKChNjexfPj9p+udOsFptLzCNd/JyKbe9y8f+M2pe3kw6G45ekPLRme7+k5Vz5VVL9pRp+ReZ49Vq/ajJz7sfHL5X6aBfHQ4sbXJ9fefqiVjIaDd65BwAAADLB4B4AAADIBIN7AAAAIBMWwugme07ibYyrEp52sV+7PuanZE9VbNvq/eEb4Nb0zvku9gX+ddQ+0PX5efRp4lblbfJz7N/i4qOjtj/Tjnbxv7SS0RaKPMevdLGfgx/b6OKdW85l9Fo9x1Oo98dd/FIX+zXm947a97SYx8tcHOflb11ygIuHuRSsBq/pkl4ftd/p+rZ2cbxW/Qtd37dby6g7s72m8849AAAAkAkG9wAAAEAmGNwDAAAAmWCde8fPja9yQv0mf+Cna1fNwfd9VXP50bKpqP30mm3j9fQnY4492rSti/3i1q+reOzZLvYTiTG457r44WPJAmPib3vS5jz72FUVfb90cYdz7CFpgYvjsZmfY++dHrUnYY79XNW+c29mu5jZhWZ2hZmtNbOj+l9fZGbnm9n6/v8Lu08XXaPe5aHmZaHeZaHe5aHmmMu0nHslHRNCWCppH0mvNbOlklZKuiCEsIekC/oxJh/1Lg81Lwv1Lgv1Lg81L1zttJwQwkb1FwgLIdxhZldKGwFcdwAAB51JREFU2km9uz5P9Tf7pHorQ765kyxHaKrBtl1N4fE5rKqJ21RavbcQr1k2v2bbr3aZyOhMRM39Z6tvcrGfIhM73cWbXHxb1L6jJo+HRG1/T/kn1Dw2norznJptOzQR9W7iCBd/yMXzGuxr/XCppCi7ejt+HcBRLSW9Z8Vxx7Wc9Wa51/z5Ln6Vi6t+HXjHDplLqhpdUGtmu6v3K+z7khb3f4Ck3q/Lxa1mhrGj3uWh5mWh3mWh3uWh5mWa8wW1ZraNpDMlHR1CuN3sj3+bhhDCbDc6MLMjJR05bKIYLepdHmpeFupdFupdHmperjm9c29m89X7AflMCOGL/S/faGZL+v1LJN0002NDCCeHEPZq+65p6A71Lg81Lwv1Lgv1Lg81L1vtO/fW+1PvFElXhhDeH3WtkbRc0kn9/8/qJMMR88tOHh+1p4bY7/H1myShtHrrSS4+Imr79zRudvFFrWczFhNR83918YtdHE9y9XU7qmbfP47aVevbSdLBUdtfB+CP+0UXL6/Z94hMRL2beLeLt2nwWL+Erf+5ykB29Xb8aecvlYrfwfz9EMfxg6XHV+TxgyGO04bcau7nDh3j4qq/QG538X8On85EmMu0nKdK+jtJPzazS/tfO069H44zzGyFpGslvaCbFDFi1Ls81Lws1Lss1Ls81Lxwc1kt51ua/eLvZ7abDsaNepeHmpeFepeFepeHmqPRajkAAAAA0jXn1XJKMV0TV5mK2nVz7L9R0bf/EDmgxv1d/B4XP7jisYe2nAvm7h9dfJCL/3SIfT82aj+mweMuc/EHXPx/XHxPg32jO7+K2v7n6O5RJoJBXF/T/ywXx/Oz//cQx32Xi1/j4iuj9mFDHAc9fxK1V7u+uqt874ra33N9pfwa5517AAAAIBMM7gEAAIBMMC2nRdOztJGQp7r4Lyq2fZ+LL2k5F8zd5S5+oovnRe1nu763u/jLLo4/4/XTsk538X9V7OcOYRz8Mqkn1Gz/kajNNJyJ41c+9TPyXuXieDqNn5Kx2sWPiNoHuL4/d7E//ePj3CkM69VR+8CGj10Rtb/QQi6TiHfuAQAAgEwwuAcAAAAyweAeAAAAyISF4G/e3OHBzEZ3MDQWQpjtphcDSbLeD3PxVS6Ol1x8m+u7t/10xuziEELdqmKNJFlz/EER5zhirZ7jKdZ7vouf5+JTovYC1+dPhvjJfc31fcbFn6pPbRwm9jXdX+MQryTs6+Zd6uKnR+3cVyCe7TWdd+4BAACATDC4BwAAADLB4B4AAADIBOvcoyw/czFnAABMrN+62N+awsdIw2NdfJqLq+bZv9nFn3Nx7vPs54J37gEAAIBMMLgHAAAAMsHgHgAAAMgEM44BAAAwMje7+BoX7xi1f+z6Pu/iDW0klBneuQcAAAAyweAeAAAAyATTcgAAADAyG12871iyyBfv3AMAAACZYHAPAAAAZILBPQAAAJCJUc+5v0XStZJ26LdTUnpOu3WwT+rdzKhz6qrmd4nv7VzkUm/O8bmb9JpT72Ymvd4Sr+lNJFNvCyGMMI/+Qc1+GELYa+QHrkBO3UnxeZBTd1J8HuTUrRSfCzl1J8XnQU7dSfF5kFM1puUAAAAAmWBwDwAAAGRiXIP7k8d03Crk1J0Unwc5dSfF50FO3UrxuZBTd1J8HuTUnRSfBzlVGMucewAAAADtY1oOAAAAkImRDu7NbJmZrTOzq81s5SiPHeVwqpndZGaXR19bZGbnm9n6/v8LR5zTLmZ2oZldYWZrzeyoFPJqAzWfMR/q3W0OSdW7f/wsa55Cvft5JFXzXOstpVHz1OrdP36WNafes+aUdL1HNrg3s3mSPiLpWZKWSjrczJaO6viR1ZKWua+tlHRBCGEPSRf041G6V9IxIYSlkvaR9Nr+92bceQ2Fms+KendrtdKqt5RhzROqt5RezbOrt5RUzVcrrXpLGdaceldKu94hhJH8k7SvpPOi+C2S3jKq47tcdpd0eRSvk7Sk314iad048oryOUvSganlRc2pN/Wm5inWO/Wa51Dv1Gqecr1zqTn1ntx6j3Jazk6Sro/iDf2vpWBxCGFjv71J0uJxJWJmu0t6gqTvK6G8BkTNa1DvkUnme5tRzVOut5TI9zajektp1zyZ721GNafec5Bivbmg1gm9P7fGsoSQmW0j6UxJR4cQbk8lr9yN63tLvceDc7w8nONl4RwvC/Xe0igH9zdI2iWKd+5/LQU3mtkSSer/f9OoEzCz+er9gHwmhPDFVPIaEjWfBfUeubF/bzOsecr1ljjHu5Byzcf+vc2w5tS7Qsr1HuXg/iJJe5jZQ81sK0mHSVozwuNXWSNpeb+9XL25UyNjZibpFElXhhDen0peLaDmM6DeY8E53r6U6y1xjnch5ZpzjrePes8i+XqP+IKDgyVdJemnkt46josMJH1W0kZJv1Vv/tgKSdurd1Xzeklfk7RoxDk9Tb2Pbn4k6dL+v4PHnRc1p97Um5qnXu8Ua55rvVOpeWr1zrnm1Hsy680dagEAAIBMcEEtAAAAkAkG9wAAAEAmGNwDAAAAmWBwDwAAAGSCwT0AAACQCQb3AAAAQCYY3AMAAACZYHAPAAAAZOL/AQByCLDDHq9YAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(3 * 28 * 28, 512)\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.fc3 = nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 3 * 28 * 28)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    logits = self.fc3(x).flatten()\n",
        "    return logits\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
        "    self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "    self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "    self.fc2 = nn.Linear(500, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = x.view(-1, 4 * 4 * 50)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    logits = self.fc2(x).flatten()\n",
        "    return logits"
      ],
      "metadata": {
        "id": "7y3qLv0o9Ykt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, device, test_loader, set_name=\"test set\"):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device).float()\n",
        "      output = model(data)\n",
        "      test_loss += F.binary_cross_entropy_with_logits(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = torch.where(torch.gt(output, torch.Tensor([0.0]).to(device)),\n",
        "                         torch.Tensor([1.0]).to(device),\n",
        "                         torch.Tensor([0.0]).to(device))  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  print('\\nPerformance on {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "    set_name, test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "  return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "\n",
        "def erm_train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.binary_cross_entropy_with_logits(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "               100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def train_and_test_erm():\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "  all_train_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='all_train',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='test', transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "    ])),\n",
        "    batch_size=1000, shuffle=True, **kwargs)\n",
        "\n",
        "  model = ConvNet().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  for epoch in range(1, 30):\n",
        "    erm_train(model, device, all_train_loader, optimizer, epoch)\n",
        "    test_model(model, device, all_train_loader, set_name='train set')\n",
        "    test_model(model, device, test_loader)\n",
        "\n",
        "train_and_test_erm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "YbGzndbu9cMA",
        "outputId": "1dfce47f-531d-449a-8201-08e3a642e68d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 0.696403\n",
            "Train Epoch: 1 [640/40000 (2%)]\tLoss: 0.537798\n",
            "Train Epoch: 1 [1280/40000 (3%)]\tLoss: 0.560925\n",
            "Train Epoch: 1 [1920/40000 (5%)]\tLoss: 0.519479\n",
            "Train Epoch: 1 [2560/40000 (6%)]\tLoss: 0.483404\n",
            "Train Epoch: 1 [3200/40000 (8%)]\tLoss: 0.521695\n",
            "Train Epoch: 1 [3840/40000 (10%)]\tLoss: 0.493725\n",
            "Train Epoch: 1 [4480/40000 (11%)]\tLoss: 0.447846\n",
            "Train Epoch: 1 [5120/40000 (13%)]\tLoss: 0.351262\n",
            "Train Epoch: 1 [5760/40000 (14%)]\tLoss: 0.489206\n",
            "Train Epoch: 1 [6400/40000 (16%)]\tLoss: 0.484357\n",
            "Train Epoch: 1 [7040/40000 (18%)]\tLoss: 0.384684\n",
            "Train Epoch: 1 [7680/40000 (19%)]\tLoss: 0.419921\n",
            "Train Epoch: 1 [8320/40000 (21%)]\tLoss: 0.444990\n",
            "Train Epoch: 1 [8960/40000 (22%)]\tLoss: 0.390235\n",
            "Train Epoch: 1 [9600/40000 (24%)]\tLoss: 0.431205\n",
            "Train Epoch: 1 [10240/40000 (26%)]\tLoss: 0.481144\n",
            "Train Epoch: 1 [10880/40000 (27%)]\tLoss: 0.467739\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4f8641d0d40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtrain_and_test_erm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-4f8641d0d40a>\u001b[0m in \u001b[0;36mtrain_and_test_erm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0merm_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4f8641d0d40a>\u001b[0m in \u001b[0;36merm_train\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e52b31f04023>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vrex_penalty(losses):\n",
        "\n",
        "  losses = torch.tensor(losses)\n",
        "  mean = losses.mean()\n",
        "  penalty = ((losses - mean) ** 2).mean()\n",
        "  return penalty\n",
        "\n",
        "def VREx_train(model, device, train_loaders, optimizer, epoch):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loaders = [iter(x) for x in train_loaders]\n",
        "  \n",
        "  batch_idx = 0\n",
        "  penalty_weight = 10000\n",
        "  penalty_anneal_iters = 40\n",
        "  regularizer_weight = 0.001\n",
        "\n",
        "  while True:\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    error = 0\n",
        "    penalty = 0\n",
        "    losses = []\n",
        "    weight_norm = torch.tensor(0.).cuda()\n",
        "    for loader in train_loaders:\n",
        "      data, target = next(loader, (None, None))\n",
        "      if data is None:\n",
        "        return\n",
        "\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      loss_erm = F.binary_cross_entropy_with_logits(output, target.float(), reduction = 'none')\n",
        "      losses.append(loss_erm.mean())\n",
        "      error += loss_erm.mean()\n",
        "\n",
        "    #VREx_penalty = calculate_vrex_penalty(losses)\n",
        "    VREx_penalty = torch.stack([losses[0], losses[1]]).std()**2\n",
        "    p_weight = (penalty_weight if epoch >= penalty_anneal_iters else 1.0)\n",
        "    \n",
        "    for w in model.parameters():\n",
        "      weight_norm += w.norm().pow(2)\n",
        "\n",
        "    final_loss = error + p_weight * VREx_penalty + regularizer_weight * weight_norm\n",
        "    if p_weight > 1.0:\n",
        "      final_loss /= p_weight\n",
        "\n",
        "    final_loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 2 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tFinal loss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loaders[0]),\n",
        "               100. * batch_idx / len(train_loaders[0]), final_loss.item()))\n",
        "      \n",
        "    batch_idx += 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Aa2jrFJv9g2T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_VREx():\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "  kwargs = {'num_workers' : 1, 'pin_memory' : True} if use_cuda else {}\n",
        "  \n",
        "  train1_loader = torch.utils.data.DataLoader(\n",
        "      ColoredMNIST(root = './data', env = 'train1',\n",
        "                   transform = transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "      batch_size = 2000,\n",
        "      shuffle = True,\n",
        "      **kwargs\n",
        "  )\n",
        "\n",
        "  train2_loader = torch.utils.data.DataLoader(\n",
        "      ColoredMNIST(root = './data', env = 'train2',\n",
        "                   transform = transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])\n",
        "                   ),\n",
        "       batch_size = 2000,\n",
        "       shuffle = True,\n",
        "       **kwargs\n",
        "\n",
        "  )\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      ColoredMNIST(root = './data', env = 'test',\n",
        "                   transform = transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "      batch_size = 1000,\n",
        "      shuffle = True,\n",
        "      **kwargs\n",
        "  )\n",
        "\n",
        "  model = ConvNet().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "  alpha = 0.2\n",
        "  gamma = 0.1\n",
        "\n",
        "  for epoch in range(1, 100):\n",
        "    VREx_train(model, device, [train1_loader, train2_loader], optimizer, epoch)\n",
        "    train1_acc = test_model(model, device, train1_loader, set_name = 'train1 set')\n",
        "    train2_acc = test_model(model, device, train2_loader, set_name = 'train2 set')\n",
        "    test_acc = test_model(model, device, test_loader)\n",
        "    if train1_acc > 75 and train2_acc > 75 and test_acc > 70:\n",
        "      print('found acceptable values. stopping training.')\n",
        "      return\n"
      ],
      "metadata": {
        "id": "k0IgvydWkDzY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_test_VREx()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyvQMazlC0Mg",
        "outputId": "1d67503a-c85c-45d1-8ba9-e170e97b513b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Train Epoch: 1 [0/10 (0%)]\tFinal loss: 1.589984\n",
            "Train Epoch: 1 [4000/10 (20%)]\tFinal loss: 1.123877\n",
            "Train Epoch: 1 [8000/10 (40%)]\tFinal loss: 1.192150\n",
            "Train Epoch: 1 [12000/10 (60%)]\tFinal loss: 1.078732\n",
            "Train Epoch: 1 [16000/10 (80%)]\tFinal loss: 0.999486\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5035, Accuracy: 15963/20000 (79.81%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3770, Accuracy: 17970/20000 (89.85%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3767, Accuracy: 2018/20000 (10.09%)\n",
            "\n",
            "Train Epoch: 2 [0/10 (0%)]\tFinal loss: 1.045393\n",
            "Train Epoch: 2 [4000/10 (20%)]\tFinal loss: 1.004841\n",
            "Train Epoch: 2 [8000/10 (40%)]\tFinal loss: 0.962440\n",
            "Train Epoch: 2 [12000/10 (60%)]\tFinal loss: 0.986197\n",
            "Train Epoch: 2 [16000/10 (80%)]\tFinal loss: 0.985666\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4886, Accuracy: 15970/20000 (79.85%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3280, Accuracy: 17929/20000 (89.64%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.6108, Accuracy: 2226/20000 (11.13%)\n",
            "\n",
            "Train Epoch: 3 [0/10 (0%)]\tFinal loss: 0.965581\n",
            "Train Epoch: 3 [4000/10 (20%)]\tFinal loss: 0.955660\n",
            "Train Epoch: 3 [8000/10 (40%)]\tFinal loss: 0.932062\n",
            "Train Epoch: 3 [12000/10 (60%)]\tFinal loss: 0.965211\n",
            "Train Epoch: 3 [16000/10 (80%)]\tFinal loss: 0.920932\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4879, Accuracy: 15967/20000 (79.83%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3131, Accuracy: 17965/20000 (89.83%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.7095, Accuracy: 2061/20000 (10.30%)\n",
            "\n",
            "Train Epoch: 4 [0/10 (0%)]\tFinal loss: 0.911374\n",
            "Train Epoch: 4 [4000/10 (20%)]\tFinal loss: 0.901315\n",
            "Train Epoch: 4 [8000/10 (40%)]\tFinal loss: 0.904751\n",
            "Train Epoch: 4 [12000/10 (60%)]\tFinal loss: 0.899254\n",
            "Train Epoch: 4 [16000/10 (80%)]\tFinal loss: 0.873410\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4637, Accuracy: 15954/20000 (79.77%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3143, Accuracy: 17912/20000 (89.56%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.5290, Accuracy: 2346/20000 (11.73%)\n",
            "\n",
            "Train Epoch: 5 [0/10 (0%)]\tFinal loss: 0.906447\n",
            "Train Epoch: 5 [4000/10 (20%)]\tFinal loss: 0.885024\n",
            "Train Epoch: 5 [8000/10 (40%)]\tFinal loss: 0.862867\n",
            "Train Epoch: 5 [12000/10 (60%)]\tFinal loss: 0.895099\n",
            "Train Epoch: 5 [16000/10 (80%)]\tFinal loss: 0.849525\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4511, Accuracy: 15970/20000 (79.85%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3101, Accuracy: 17938/20000 (89.69%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4648, Accuracy: 2179/20000 (10.89%)\n",
            "\n",
            "Train Epoch: 6 [0/10 (0%)]\tFinal loss: 0.898861\n",
            "Train Epoch: 6 [4000/10 (20%)]\tFinal loss: 0.850187\n",
            "Train Epoch: 6 [8000/10 (40%)]\tFinal loss: 0.837733\n",
            "Train Epoch: 6 [12000/10 (60%)]\tFinal loss: 0.820759\n",
            "Train Epoch: 6 [16000/10 (80%)]\tFinal loss: 0.853365\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4423, Accuracy: 15966/20000 (79.83%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3139, Accuracy: 17871/20000 (89.36%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3715, Accuracy: 2468/20000 (12.34%)\n",
            "\n",
            "Train Epoch: 7 [0/10 (0%)]\tFinal loss: 0.852543\n",
            "Train Epoch: 7 [4000/10 (20%)]\tFinal loss: 0.823873\n",
            "Train Epoch: 7 [8000/10 (40%)]\tFinal loss: 0.851072\n",
            "Train Epoch: 7 [12000/10 (60%)]\tFinal loss: 0.842268\n",
            "Train Epoch: 7 [16000/10 (80%)]\tFinal loss: 0.828918\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4454, Accuracy: 15972/20000 (79.86%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3017, Accuracy: 17945/20000 (89.72%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4830, Accuracy: 2204/20000 (11.02%)\n",
            "\n",
            "Train Epoch: 8 [0/10 (0%)]\tFinal loss: 0.845597\n",
            "Train Epoch: 8 [4000/10 (20%)]\tFinal loss: 0.836397\n",
            "Train Epoch: 8 [8000/10 (40%)]\tFinal loss: 0.838098\n",
            "Train Epoch: 8 [12000/10 (60%)]\tFinal loss: 0.832553\n",
            "Train Epoch: 8 [16000/10 (80%)]\tFinal loss: 0.792611\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4432, Accuracy: 15964/20000 (79.82%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3011, Accuracy: 17946/20000 (89.73%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4720, Accuracy: 2216/20000 (11.08%)\n",
            "\n",
            "Train Epoch: 9 [0/10 (0%)]\tFinal loss: 0.834396\n",
            "Train Epoch: 9 [4000/10 (20%)]\tFinal loss: 0.841723\n",
            "Train Epoch: 9 [8000/10 (40%)]\tFinal loss: 0.812923\n",
            "Train Epoch: 9 [12000/10 (60%)]\tFinal loss: 0.806463\n",
            "Train Epoch: 9 [16000/10 (80%)]\tFinal loss: 0.806485\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4374, Accuracy: 15969/20000 (79.84%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3016, Accuracy: 17922/20000 (89.61%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4272, Accuracy: 2272/20000 (11.36%)\n",
            "\n",
            "Train Epoch: 10 [0/10 (0%)]\tFinal loss: 0.809761\n",
            "Train Epoch: 10 [4000/10 (20%)]\tFinal loss: 0.806815\n",
            "Train Epoch: 10 [8000/10 (40%)]\tFinal loss: 0.824073\n",
            "Train Epoch: 10 [12000/10 (60%)]\tFinal loss: 0.784116\n",
            "Train Epoch: 10 [16000/10 (80%)]\tFinal loss: 0.829207\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4345, Accuracy: 15960/20000 (79.80%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3054, Accuracy: 17890/20000 (89.45%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3858, Accuracy: 2561/20000 (12.80%)\n",
            "\n",
            "Train Epoch: 11 [0/10 (0%)]\tFinal loss: 0.788686\n",
            "Train Epoch: 11 [4000/10 (20%)]\tFinal loss: 0.810261\n",
            "Train Epoch: 11 [8000/10 (40%)]\tFinal loss: 0.830966\n",
            "Train Epoch: 11 [12000/10 (60%)]\tFinal loss: 0.790434\n",
            "Train Epoch: 11 [16000/10 (80%)]\tFinal loss: 0.804390\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4328, Accuracy: 15988/20000 (79.94%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3034, Accuracy: 17917/20000 (89.58%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3820, Accuracy: 2329/20000 (11.64%)\n",
            "\n",
            "Train Epoch: 12 [0/10 (0%)]\tFinal loss: 0.819884\n",
            "Train Epoch: 12 [4000/10 (20%)]\tFinal loss: 0.767181\n",
            "Train Epoch: 12 [8000/10 (40%)]\tFinal loss: 0.786346\n",
            "Train Epoch: 12 [12000/10 (60%)]\tFinal loss: 0.805430\n",
            "Train Epoch: 12 [16000/10 (80%)]\tFinal loss: 0.803578\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4434, Accuracy: 15971/20000 (79.86%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3074, Accuracy: 17791/20000 (88.95%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4539, Accuracy: 2895/20000 (14.47%)\n",
            "\n",
            "Train Epoch: 13 [0/10 (0%)]\tFinal loss: 0.804432\n",
            "Train Epoch: 13 [4000/10 (20%)]\tFinal loss: 0.792322\n",
            "Train Epoch: 13 [8000/10 (40%)]\tFinal loss: 0.762065\n",
            "Train Epoch: 13 [12000/10 (60%)]\tFinal loss: 0.805675\n",
            "Train Epoch: 13 [16000/10 (80%)]\tFinal loss: 0.784989\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4299, Accuracy: 15997/20000 (79.98%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3031, Accuracy: 17865/20000 (89.33%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3772, Accuracy: 2556/20000 (12.78%)\n",
            "\n",
            "Train Epoch: 14 [0/10 (0%)]\tFinal loss: 0.798083\n",
            "Train Epoch: 14 [4000/10 (20%)]\tFinal loss: 0.802616\n",
            "Train Epoch: 14 [8000/10 (40%)]\tFinal loss: 0.778495\n",
            "Train Epoch: 14 [12000/10 (60%)]\tFinal loss: 0.787148\n",
            "Train Epoch: 14 [16000/10 (80%)]\tFinal loss: 0.774755\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4372, Accuracy: 16016/20000 (80.08%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2997, Accuracy: 17878/20000 (89.39%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4634, Accuracy: 2541/20000 (12.71%)\n",
            "\n",
            "Train Epoch: 15 [0/10 (0%)]\tFinal loss: 0.815712\n",
            "Train Epoch: 15 [4000/10 (20%)]\tFinal loss: 0.782250\n",
            "Train Epoch: 15 [8000/10 (40%)]\tFinal loss: 0.773612\n",
            "Train Epoch: 15 [12000/10 (60%)]\tFinal loss: 0.803827\n",
            "Train Epoch: 15 [16000/10 (80%)]\tFinal loss: 0.774678\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4328, Accuracy: 15972/20000 (79.86%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2924, Accuracy: 17957/20000 (89.78%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4794, Accuracy: 2230/20000 (11.15%)\n",
            "\n",
            "Train Epoch: 16 [0/10 (0%)]\tFinal loss: 0.810098\n",
            "Train Epoch: 16 [4000/10 (20%)]\tFinal loss: 0.771966\n",
            "Train Epoch: 16 [8000/10 (40%)]\tFinal loss: 0.756784\n",
            "Train Epoch: 16 [12000/10 (60%)]\tFinal loss: 0.804052\n",
            "Train Epoch: 16 [16000/10 (80%)]\tFinal loss: 0.761790\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4249, Accuracy: 15993/20000 (79.97%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2977, Accuracy: 17924/20000 (89.62%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3869, Accuracy: 2355/20000 (11.78%)\n",
            "\n",
            "Train Epoch: 17 [0/10 (0%)]\tFinal loss: 0.780855\n",
            "Train Epoch: 17 [4000/10 (20%)]\tFinal loss: 0.770953\n",
            "Train Epoch: 17 [8000/10 (40%)]\tFinal loss: 0.775127\n",
            "Train Epoch: 17 [12000/10 (60%)]\tFinal loss: 0.775667\n",
            "Train Epoch: 17 [16000/10 (80%)]\tFinal loss: 0.766451\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4273, Accuracy: 15997/20000 (79.98%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2932, Accuracy: 17933/20000 (89.67%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4452, Accuracy: 2375/20000 (11.88%)\n",
            "\n",
            "Train Epoch: 18 [0/10 (0%)]\tFinal loss: 0.735950\n",
            "Train Epoch: 18 [4000/10 (20%)]\tFinal loss: 0.759647\n",
            "Train Epoch: 18 [8000/10 (40%)]\tFinal loss: 0.775679\n",
            "Train Epoch: 18 [12000/10 (60%)]\tFinal loss: 0.759457\n",
            "Train Epoch: 18 [16000/10 (80%)]\tFinal loss: 0.801497\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4209, Accuracy: 16002/20000 (80.01%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2983, Accuracy: 17930/20000 (89.65%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3561, Accuracy: 2483/20000 (12.41%)\n",
            "\n",
            "Train Epoch: 19 [0/10 (0%)]\tFinal loss: 0.766367\n",
            "Train Epoch: 19 [4000/10 (20%)]\tFinal loss: 0.777919\n",
            "Train Epoch: 19 [8000/10 (40%)]\tFinal loss: 0.731925\n",
            "Train Epoch: 19 [12000/10 (60%)]\tFinal loss: 0.773664\n",
            "Train Epoch: 19 [16000/10 (80%)]\tFinal loss: 0.783196\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4250, Accuracy: 15986/20000 (79.93%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2911, Accuracy: 17947/20000 (89.73%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4487, Accuracy: 2326/20000 (11.63%)\n",
            "\n",
            "Train Epoch: 20 [0/10 (0%)]\tFinal loss: 0.768173\n",
            "Train Epoch: 20 [4000/10 (20%)]\tFinal loss: 0.723758\n",
            "Train Epoch: 20 [8000/10 (40%)]\tFinal loss: 0.768109\n",
            "Train Epoch: 20 [12000/10 (60%)]\tFinal loss: 0.771502\n",
            "Train Epoch: 20 [16000/10 (80%)]\tFinal loss: 0.775498\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4248, Accuracy: 15982/20000 (79.91%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2879, Accuracy: 17977/20000 (89.89%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4767, Accuracy: 2274/20000 (11.37%)\n",
            "\n",
            "Train Epoch: 21 [0/10 (0%)]\tFinal loss: 0.782361\n",
            "Train Epoch: 21 [4000/10 (20%)]\tFinal loss: 0.761745\n",
            "Train Epoch: 21 [8000/10 (40%)]\tFinal loss: 0.756800\n",
            "Train Epoch: 21 [12000/10 (60%)]\tFinal loss: 0.760252\n",
            "Train Epoch: 21 [16000/10 (80%)]\tFinal loss: 0.763353\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4220, Accuracy: 16025/20000 (80.12%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2906, Accuracy: 17887/20000 (89.44%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4428, Accuracy: 2656/20000 (13.28%)\n",
            "\n",
            "Train Epoch: 22 [0/10 (0%)]\tFinal loss: 0.755357\n",
            "Train Epoch: 22 [4000/10 (20%)]\tFinal loss: 0.732303\n",
            "Train Epoch: 22 [8000/10 (40%)]\tFinal loss: 0.748627\n",
            "Train Epoch: 22 [12000/10 (60%)]\tFinal loss: 0.758240\n",
            "Train Epoch: 22 [16000/10 (80%)]\tFinal loss: 0.744806\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4195, Accuracy: 16021/20000 (80.11%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2950, Accuracy: 17854/20000 (89.27%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4028, Accuracy: 2940/20000 (14.70%)\n",
            "\n",
            "Train Epoch: 23 [0/10 (0%)]\tFinal loss: 0.734020\n",
            "Train Epoch: 23 [4000/10 (20%)]\tFinal loss: 0.734827\n",
            "Train Epoch: 23 [8000/10 (40%)]\tFinal loss: 0.768875\n",
            "Train Epoch: 23 [12000/10 (60%)]\tFinal loss: 0.766421\n",
            "Train Epoch: 23 [16000/10 (80%)]\tFinal loss: 0.769204\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4316, Accuracy: 16026/20000 (80.13%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2868, Accuracy: 17938/20000 (89.69%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.5581, Accuracy: 2324/20000 (11.62%)\n",
            "\n",
            "Train Epoch: 24 [0/10 (0%)]\tFinal loss: 0.791112\n",
            "Train Epoch: 24 [4000/10 (20%)]\tFinal loss: 0.734089\n",
            "Train Epoch: 24 [8000/10 (40%)]\tFinal loss: 0.742342\n",
            "Train Epoch: 24 [12000/10 (60%)]\tFinal loss: 0.751612\n",
            "Train Epoch: 24 [16000/10 (80%)]\tFinal loss: 0.733744\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4128, Accuracy: 16109/20000 (80.55%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3004, Accuracy: 17858/20000 (89.29%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3183, Accuracy: 3090/20000 (15.45%)\n",
            "\n",
            "Train Epoch: 25 [0/10 (0%)]\tFinal loss: 0.733795\n",
            "Train Epoch: 25 [4000/10 (20%)]\tFinal loss: 0.760330\n",
            "Train Epoch: 25 [8000/10 (40%)]\tFinal loss: 0.748794\n",
            "Train Epoch: 25 [12000/10 (60%)]\tFinal loss: 0.779719\n",
            "Train Epoch: 25 [16000/10 (80%)]\tFinal loss: 0.748307\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4177, Accuracy: 16022/20000 (80.11%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2837, Accuracy: 17972/20000 (89.86%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4803, Accuracy: 2307/20000 (11.54%)\n",
            "\n",
            "Train Epoch: 26 [0/10 (0%)]\tFinal loss: 0.789980\n",
            "Train Epoch: 26 [4000/10 (20%)]\tFinal loss: 0.747662\n",
            "Train Epoch: 26 [8000/10 (40%)]\tFinal loss: 0.747758\n",
            "Train Epoch: 26 [12000/10 (60%)]\tFinal loss: 0.720979\n",
            "Train Epoch: 26 [16000/10 (80%)]\tFinal loss: 0.769217\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4088, Accuracy: 16116/20000 (80.58%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2905, Accuracy: 17908/20000 (89.54%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3738, Accuracy: 2802/20000 (14.01%)\n",
            "\n",
            "Train Epoch: 27 [0/10 (0%)]\tFinal loss: 0.727653\n",
            "Train Epoch: 27 [4000/10 (20%)]\tFinal loss: 0.750227\n",
            "Train Epoch: 27 [8000/10 (40%)]\tFinal loss: 0.769244\n",
            "Train Epoch: 27 [12000/10 (60%)]\tFinal loss: 0.732443\n",
            "Train Epoch: 27 [16000/10 (80%)]\tFinal loss: 0.725443\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4075, Accuracy: 16106/20000 (80.53%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2893, Accuracy: 17932/20000 (89.66%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3762, Accuracy: 2637/20000 (13.19%)\n",
            "\n",
            "Train Epoch: 28 [0/10 (0%)]\tFinal loss: 0.711795\n",
            "Train Epoch: 28 [4000/10 (20%)]\tFinal loss: 0.749551\n",
            "Train Epoch: 28 [8000/10 (40%)]\tFinal loss: 0.762398\n",
            "Train Epoch: 28 [12000/10 (60%)]\tFinal loss: 0.746178\n",
            "Train Epoch: 28 [16000/10 (80%)]\tFinal loss: 0.710995\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4113, Accuracy: 16097/20000 (80.48%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2845, Accuracy: 17946/20000 (89.73%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4483, Accuracy: 2691/20000 (13.46%)\n",
            "\n",
            "Train Epoch: 29 [0/10 (0%)]\tFinal loss: 0.721987\n",
            "Train Epoch: 29 [4000/10 (20%)]\tFinal loss: 0.702321\n",
            "Train Epoch: 29 [8000/10 (40%)]\tFinal loss: 0.727239\n",
            "Train Epoch: 29 [12000/10 (60%)]\tFinal loss: 0.750765\n",
            "Train Epoch: 29 [16000/10 (80%)]\tFinal loss: 0.754600\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4079, Accuracy: 16109/20000 (80.55%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2853, Accuracy: 17953/20000 (89.77%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4336, Accuracy: 2744/20000 (13.72%)\n",
            "\n",
            "Train Epoch: 30 [0/10 (0%)]\tFinal loss: 0.731266\n",
            "Train Epoch: 30 [4000/10 (20%)]\tFinal loss: 0.748521\n",
            "Train Epoch: 30 [8000/10 (40%)]\tFinal loss: 0.774675\n",
            "Train Epoch: 30 [12000/10 (60%)]\tFinal loss: 0.721023\n",
            "Train Epoch: 30 [16000/10 (80%)]\tFinal loss: 0.727244\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4015, Accuracy: 16207/20000 (81.03%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2873, Accuracy: 17937/20000 (89.69%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3803, Accuracy: 3027/20000 (15.13%)\n",
            "\n",
            "Train Epoch: 31 [0/10 (0%)]\tFinal loss: 0.739430\n",
            "Train Epoch: 31 [4000/10 (20%)]\tFinal loss: 0.763328\n",
            "Train Epoch: 31 [8000/10 (40%)]\tFinal loss: 0.694561\n",
            "Train Epoch: 31 [12000/10 (60%)]\tFinal loss: 0.735030\n",
            "Train Epoch: 31 [16000/10 (80%)]\tFinal loss: 0.734539\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4058, Accuracy: 16227/20000 (81.14%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2879, Accuracy: 17871/20000 (89.36%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4224, Accuracy: 3549/20000 (17.75%)\n",
            "\n",
            "Train Epoch: 32 [0/10 (0%)]\tFinal loss: 0.740444\n",
            "Train Epoch: 32 [4000/10 (20%)]\tFinal loss: 0.751799\n",
            "Train Epoch: 32 [8000/10 (40%)]\tFinal loss: 0.711489\n",
            "Train Epoch: 32 [12000/10 (60%)]\tFinal loss: 0.717650\n",
            "Train Epoch: 32 [16000/10 (80%)]\tFinal loss: 0.780943\n",
            "\n",
            "Performance on train1 set: Average loss: 0.3954, Accuracy: 16340/20000 (81.70%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2908, Accuracy: 17850/20000 (89.25%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3281, Accuracy: 3787/20000 (18.93%)\n",
            "\n",
            "Train Epoch: 33 [0/10 (0%)]\tFinal loss: 0.722313\n",
            "Train Epoch: 33 [4000/10 (20%)]\tFinal loss: 0.776083\n",
            "Train Epoch: 33 [8000/10 (40%)]\tFinal loss: 0.728365\n",
            "Train Epoch: 33 [12000/10 (60%)]\tFinal loss: 0.758163\n",
            "Train Epoch: 33 [16000/10 (80%)]\tFinal loss: 0.732928\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4010, Accuracy: 16235/20000 (81.17%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2814, Accuracy: 17917/20000 (89.58%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4493, Accuracy: 2943/20000 (14.71%)\n",
            "\n",
            "Train Epoch: 34 [0/10 (0%)]\tFinal loss: 0.733112\n",
            "Train Epoch: 34 [4000/10 (20%)]\tFinal loss: 0.720358\n",
            "Train Epoch: 34 [8000/10 (40%)]\tFinal loss: 0.733942\n",
            "Train Epoch: 34 [12000/10 (60%)]\tFinal loss: 0.706817\n",
            "Train Epoch: 34 [16000/10 (80%)]\tFinal loss: 0.750926\n",
            "\n",
            "Performance on train1 set: Average loss: 0.3972, Accuracy: 16313/20000 (81.56%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2849, Accuracy: 17881/20000 (89.41%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4008, Accuracy: 3358/20000 (16.79%)\n",
            "\n",
            "Train Epoch: 35 [0/10 (0%)]\tFinal loss: 0.711934\n",
            "Train Epoch: 35 [4000/10 (20%)]\tFinal loss: 0.764983\n",
            "Train Epoch: 35 [8000/10 (40%)]\tFinal loss: 0.723861\n",
            "Train Epoch: 35 [12000/10 (60%)]\tFinal loss: 0.711635\n",
            "Train Epoch: 35 [16000/10 (80%)]\tFinal loss: 0.772283\n",
            "\n",
            "Performance on train1 set: Average loss: 0.3913, Accuracy: 16462/20000 (82.31%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2980, Accuracy: 17787/20000 (88.94%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.2619, Accuracy: 4338/20000 (21.69%)\n",
            "\n",
            "Train Epoch: 36 [0/10 (0%)]\tFinal loss: 0.721261\n",
            "Train Epoch: 36 [4000/10 (20%)]\tFinal loss: 0.705178\n",
            "Train Epoch: 36 [8000/10 (40%)]\tFinal loss: 0.715993\n",
            "Train Epoch: 36 [12000/10 (60%)]\tFinal loss: 0.693908\n",
            "Train Epoch: 36 [16000/10 (80%)]\tFinal loss: 0.747326\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4009, Accuracy: 16250/20000 (81.25%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2774, Accuracy: 17927/20000 (89.64%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.5133, Accuracy: 3186/20000 (15.93%)\n",
            "\n",
            "Train Epoch: 37 [0/10 (0%)]\tFinal loss: 0.697943\n",
            "Train Epoch: 37 [4000/10 (20%)]\tFinal loss: 0.701933\n",
            "Train Epoch: 37 [8000/10 (40%)]\tFinal loss: 0.710684\n",
            "Train Epoch: 37 [12000/10 (60%)]\tFinal loss: 0.719212\n",
            "Train Epoch: 37 [16000/10 (80%)]\tFinal loss: 0.741614\n",
            "\n",
            "Performance on train1 set: Average loss: 0.3923, Accuracy: 16372/20000 (81.86%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2828, Accuracy: 17934/20000 (89.67%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.4073, Accuracy: 3484/20000 (17.42%)\n",
            "\n",
            "Train Epoch: 38 [0/10 (0%)]\tFinal loss: 0.693830\n",
            "Train Epoch: 38 [4000/10 (20%)]\tFinal loss: 0.705908\n",
            "Train Epoch: 38 [8000/10 (40%)]\tFinal loss: 0.706838\n",
            "Train Epoch: 38 [12000/10 (60%)]\tFinal loss: 0.739014\n",
            "Train Epoch: 38 [16000/10 (80%)]\tFinal loss: 0.745716\n",
            "\n",
            "Performance on train1 set: Average loss: 0.4014, Accuracy: 16280/20000 (81.40%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.2812, Accuracy: 17922/20000 (89.61%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.5091, Accuracy: 2958/20000 (14.79%)\n",
            "\n",
            "Train Epoch: 39 [0/10 (0%)]\tFinal loss: 0.717620\n",
            "Train Epoch: 39 [4000/10 (20%)]\tFinal loss: 0.760526\n",
            "Train Epoch: 39 [8000/10 (40%)]\tFinal loss: 0.711751\n",
            "Train Epoch: 39 [12000/10 (60%)]\tFinal loss: 0.725556\n",
            "Train Epoch: 39 [16000/10 (80%)]\tFinal loss: 0.707771\n",
            "\n",
            "Performance on train1 set: Average loss: 0.3937, Accuracy: 16402/20000 (82.01%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.3010, Accuracy: 17687/20000 (88.44%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.2860, Accuracy: 4719/20000 (23.59%)\n",
            "\n",
            "Train Epoch: 40 [0/10 (0%)]\tFinal loss: 0.005495\n",
            "Train Epoch: 40 [4000/10 (20%)]\tFinal loss: 0.002629\n",
            "Train Epoch: 40 [8000/10 (40%)]\tFinal loss: 0.001548\n",
            "Train Epoch: 40 [12000/10 (60%)]\tFinal loss: 0.000408\n",
            "Train Epoch: 40 [16000/10 (80%)]\tFinal loss: 0.000278\n",
            "\n",
            "Performance on train1 set: Average loss: 1.5132, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 1.5158, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.6875, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 41 [0/10 (0%)]\tFinal loss: 0.000538\n",
            "Train Epoch: 41 [4000/10 (20%)]\tFinal loss: 0.002728\n",
            "Train Epoch: 41 [8000/10 (40%)]\tFinal loss: 0.000339\n",
            "Train Epoch: 41 [12000/10 (60%)]\tFinal loss: 0.000332\n",
            "Train Epoch: 41 [16000/10 (80%)]\tFinal loss: 0.000346\n",
            "\n",
            "Performance on train1 set: Average loss: 1.6400, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 1.6530, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.7452, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 42 [0/10 (0%)]\tFinal loss: 0.002076\n",
            "Train Epoch: 42 [4000/10 (20%)]\tFinal loss: 0.000533\n",
            "Train Epoch: 42 [8000/10 (40%)]\tFinal loss: 0.000642\n",
            "Train Epoch: 42 [12000/10 (60%)]\tFinal loss: 0.001159\n",
            "Train Epoch: 42 [16000/10 (80%)]\tFinal loss: 0.000606\n",
            "\n",
            "Performance on train1 set: Average loss: 1.3897, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 1.3810, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.6201, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 43 [0/10 (0%)]\tFinal loss: 0.000534\n",
            "Train Epoch: 43 [4000/10 (20%)]\tFinal loss: 0.000288\n",
            "Train Epoch: 43 [8000/10 (40%)]\tFinal loss: 0.000685\n",
            "Train Epoch: 43 [12000/10 (60%)]\tFinal loss: 0.000275\n",
            "Train Epoch: 43 [16000/10 (80%)]\tFinal loss: 0.000628\n",
            "\n",
            "Performance on train1 set: Average loss: 1.2579, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 1.2432, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.5193, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 44 [0/10 (0%)]\tFinal loss: 0.000320\n",
            "Train Epoch: 44 [4000/10 (20%)]\tFinal loss: 0.000900\n",
            "Train Epoch: 44 [8000/10 (40%)]\tFinal loss: 0.000325\n",
            "Train Epoch: 44 [12000/10 (60%)]\tFinal loss: 0.001465\n",
            "Train Epoch: 44 [16000/10 (80%)]\tFinal loss: 0.001331\n",
            "\n",
            "Performance on train1 set: Average loss: 1.2296, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 1.2315, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.3813, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 45 [0/10 (0%)]\tFinal loss: 0.000256\n",
            "Train Epoch: 45 [4000/10 (20%)]\tFinal loss: 0.000587\n",
            "Train Epoch: 45 [8000/10 (40%)]\tFinal loss: 0.001914\n",
            "Train Epoch: 45 [12000/10 (60%)]\tFinal loss: 0.000376\n",
            "Train Epoch: 45 [16000/10 (80%)]\tFinal loss: 0.000337\n",
            "\n",
            "Performance on train1 set: Average loss: 1.1066, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 1.1043, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.2754, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 46 [0/10 (0%)]\tFinal loss: 0.000534\n",
            "Train Epoch: 46 [4000/10 (20%)]\tFinal loss: 0.000898\n",
            "Train Epoch: 46 [8000/10 (40%)]\tFinal loss: 0.001037\n",
            "Train Epoch: 46 [12000/10 (60%)]\tFinal loss: 0.001356\n",
            "Train Epoch: 46 [16000/10 (80%)]\tFinal loss: 0.000533\n",
            "\n",
            "Performance on train1 set: Average loss: 0.9846, Accuracy: 10105/20000 (50.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.9780, Accuracy: 10039/20000 (50.20%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.1710, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 47 [0/10 (0%)]\tFinal loss: 0.001188\n",
            "Train Epoch: 47 [4000/10 (20%)]\tFinal loss: 0.000211\n",
            "Train Epoch: 47 [8000/10 (40%)]\tFinal loss: 0.000208\n",
            "Train Epoch: 47 [12000/10 (60%)]\tFinal loss: 0.000524\n",
            "Train Epoch: 47 [16000/10 (80%)]\tFinal loss: 0.000185\n",
            "\n",
            "Performance on train1 set: Average loss: 0.9045, Accuracy: 10106/20000 (50.53%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.8990, Accuracy: 10042/20000 (50.21%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 1.0748, Accuracy: 10035/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 48 [0/10 (0%)]\tFinal loss: 0.000597\n",
            "Train Epoch: 48 [4000/10 (20%)]\tFinal loss: 0.000500\n",
            "Train Epoch: 48 [8000/10 (40%)]\tFinal loss: 0.000200\n",
            "Train Epoch: 48 [12000/10 (60%)]\tFinal loss: 0.000457\n",
            "Train Epoch: 48 [16000/10 (80%)]\tFinal loss: 0.000208\n",
            "\n",
            "Performance on train1 set: Average loss: 0.8466, Accuracy: 10108/20000 (50.54%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.8459, Accuracy: 10044/20000 (50.22%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.9799, Accuracy: 10034/20000 (50.17%)\n",
            "\n",
            "Train Epoch: 49 [0/10 (0%)]\tFinal loss: 0.000191\n",
            "Train Epoch: 49 [4000/10 (20%)]\tFinal loss: 0.002205\n",
            "Train Epoch: 49 [8000/10 (40%)]\tFinal loss: 0.000602\n",
            "Train Epoch: 49 [12000/10 (60%)]\tFinal loss: 0.000421\n",
            "Train Epoch: 49 [16000/10 (80%)]\tFinal loss: 0.000176\n",
            "\n",
            "Performance on train1 set: Average loss: 0.7439, Accuracy: 10148/20000 (50.74%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.7431, Accuracy: 10086/20000 (50.43%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.8677, Accuracy: 10020/20000 (50.10%)\n",
            "\n",
            "Train Epoch: 50 [0/10 (0%)]\tFinal loss: 0.000507\n",
            "Train Epoch: 50 [4000/10 (20%)]\tFinal loss: 0.000156\n",
            "Train Epoch: 50 [8000/10 (40%)]\tFinal loss: 0.000315\n",
            "Train Epoch: 50 [12000/10 (60%)]\tFinal loss: 0.000197\n",
            "Train Epoch: 50 [16000/10 (80%)]\tFinal loss: 0.000203\n",
            "\n",
            "Performance on train1 set: Average loss: 0.6587, Accuracy: 10606/20000 (53.03%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.6547, Accuracy: 10597/20000 (52.98%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.7908, Accuracy: 9955/20000 (49.77%)\n",
            "\n",
            "Train Epoch: 51 [0/10 (0%)]\tFinal loss: 0.000336\n",
            "Train Epoch: 51 [4000/10 (20%)]\tFinal loss: 0.000794\n",
            "Train Epoch: 51 [8000/10 (40%)]\tFinal loss: 0.000168\n",
            "Train Epoch: 51 [12000/10 (60%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 51 [16000/10 (80%)]\tFinal loss: 0.000161\n",
            "\n",
            "Performance on train1 set: Average loss: 0.6111, Accuracy: 12177/20000 (60.88%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.6074, Accuracy: 12248/20000 (61.24%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.7296, Accuracy: 10525/20000 (52.62%)\n",
            "\n",
            "Train Epoch: 52 [0/10 (0%)]\tFinal loss: 0.000142\n",
            "Train Epoch: 52 [4000/10 (20%)]\tFinal loss: 0.000149\n",
            "Train Epoch: 52 [8000/10 (40%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 52 [12000/10 (60%)]\tFinal loss: 0.000264\n",
            "Train Epoch: 52 [16000/10 (80%)]\tFinal loss: 0.000139\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5971, Accuracy: 13560/20000 (67.80%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5983, Accuracy: 13309/20000 (66.55%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6729, Accuracy: 12725/20000 (63.62%)\n",
            "\n",
            "Train Epoch: 53 [0/10 (0%)]\tFinal loss: 0.000158\n",
            "Train Epoch: 53 [4000/10 (20%)]\tFinal loss: 0.000147\n",
            "Train Epoch: 53 [8000/10 (40%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 53 [12000/10 (60%)]\tFinal loss: 0.000171\n",
            "Train Epoch: 53 [16000/10 (80%)]\tFinal loss: 0.000165\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5881, Accuracy: 14359/20000 (71.80%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5888, Accuracy: 14182/20000 (70.91%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6595, Accuracy: 13393/20000 (66.97%)\n",
            "\n",
            "Train Epoch: 54 [0/10 (0%)]\tFinal loss: 0.000135\n",
            "Train Epoch: 54 [4000/10 (20%)]\tFinal loss: 0.000160\n",
            "Train Epoch: 54 [8000/10 (40%)]\tFinal loss: 0.000153\n",
            "Train Epoch: 54 [12000/10 (60%)]\tFinal loss: 0.000120\n",
            "Train Epoch: 54 [16000/10 (80%)]\tFinal loss: 0.000124\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5799, Accuracy: 14865/20000 (74.33%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5773, Accuracy: 14808/20000 (74.04%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6672, Accuracy: 13021/20000 (65.11%)\n",
            "\n",
            "Train Epoch: 55 [0/10 (0%)]\tFinal loss: 0.000121\n",
            "Train Epoch: 55 [4000/10 (20%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 55 [8000/10 (40%)]\tFinal loss: 0.000207\n",
            "Train Epoch: 55 [12000/10 (60%)]\tFinal loss: 0.000133\n",
            "Train Epoch: 55 [16000/10 (80%)]\tFinal loss: 0.000140\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5855, Accuracy: 14696/20000 (73.48%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5848, Accuracy: 14611/20000 (73.06%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6605, Accuracy: 13260/20000 (66.30%)\n",
            "\n",
            "Train Epoch: 56 [0/10 (0%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 56 [4000/10 (20%)]\tFinal loss: 0.000137\n",
            "Train Epoch: 56 [8000/10 (40%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 56 [12000/10 (60%)]\tFinal loss: 0.000176\n",
            "Train Epoch: 56 [16000/10 (80%)]\tFinal loss: 0.000142\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5896, Accuracy: 14593/20000 (72.97%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5903, Accuracy: 14484/20000 (72.42%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6553, Accuracy: 13397/20000 (66.98%)\n",
            "\n",
            "Train Epoch: 57 [0/10 (0%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 57 [4000/10 (20%)]\tFinal loss: 0.000185\n",
            "Train Epoch: 57 [8000/10 (40%)]\tFinal loss: 0.000247\n",
            "Train Epoch: 57 [12000/10 (60%)]\tFinal loss: 0.000141\n",
            "Train Epoch: 57 [16000/10 (80%)]\tFinal loss: 0.000141\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5869, Accuracy: 14591/20000 (72.95%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5865, Accuracy: 14480/20000 (72.40%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6618, Accuracy: 13279/20000 (66.39%)\n",
            "\n",
            "Train Epoch: 58 [0/10 (0%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 58 [4000/10 (20%)]\tFinal loss: 0.000161\n",
            "Train Epoch: 58 [8000/10 (40%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 58 [12000/10 (60%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 58 [16000/10 (80%)]\tFinal loss: 0.000136\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5860, Accuracy: 14664/20000 (73.32%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5853, Accuracy: 14599/20000 (73.00%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6628, Accuracy: 13221/20000 (66.11%)\n",
            "\n",
            "Train Epoch: 59 [0/10 (0%)]\tFinal loss: 0.000186\n",
            "Train Epoch: 59 [4000/10 (20%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 59 [8000/10 (40%)]\tFinal loss: 0.000167\n",
            "Train Epoch: 59 [12000/10 (60%)]\tFinal loss: 0.000138\n",
            "Train Epoch: 59 [16000/10 (80%)]\tFinal loss: 0.000122\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5864, Accuracy: 14671/20000 (73.36%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5855, Accuracy: 14575/20000 (72.88%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6639, Accuracy: 13194/20000 (65.97%)\n",
            "\n",
            "Train Epoch: 60 [0/10 (0%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 60 [4000/10 (20%)]\tFinal loss: 0.000161\n",
            "Train Epoch: 60 [8000/10 (40%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 60 [12000/10 (60%)]\tFinal loss: 0.000165\n",
            "Train Epoch: 60 [16000/10 (80%)]\tFinal loss: 0.000124\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5876, Accuracy: 14536/20000 (72.68%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5870, Accuracy: 14414/20000 (72.07%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6648, Accuracy: 13197/20000 (65.98%)\n",
            "\n",
            "Train Epoch: 61 [0/10 (0%)]\tFinal loss: 0.000145\n",
            "Train Epoch: 61 [4000/10 (20%)]\tFinal loss: 0.000198\n",
            "Train Epoch: 61 [8000/10 (40%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 61 [12000/10 (60%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 61 [16000/10 (80%)]\tFinal loss: 0.000126\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5899, Accuracy: 14393/20000 (71.97%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5902, Accuracy: 14248/20000 (71.24%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6642, Accuracy: 13279/20000 (66.39%)\n",
            "\n",
            "Train Epoch: 62 [0/10 (0%)]\tFinal loss: 0.000158\n",
            "Train Epoch: 62 [4000/10 (20%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 62 [8000/10 (40%)]\tFinal loss: 0.000173\n",
            "Train Epoch: 62 [12000/10 (60%)]\tFinal loss: 0.000144\n",
            "Train Epoch: 62 [16000/10 (80%)]\tFinal loss: 0.000124\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5857, Accuracy: 14679/20000 (73.39%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5847, Accuracy: 14596/20000 (72.98%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6671, Accuracy: 13153/20000 (65.77%)\n",
            "\n",
            "Train Epoch: 63 [0/10 (0%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 63 [4000/10 (20%)]\tFinal loss: 0.000137\n",
            "Train Epoch: 63 [8000/10 (40%)]\tFinal loss: 0.000121\n",
            "Train Epoch: 63 [12000/10 (60%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 63 [16000/10 (80%)]\tFinal loss: 0.000124\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5846, Accuracy: 14791/20000 (73.95%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5833, Accuracy: 14717/20000 (73.58%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6670, Accuracy: 13102/20000 (65.51%)\n",
            "\n",
            "Train Epoch: 64 [0/10 (0%)]\tFinal loss: 0.000126\n",
            "Train Epoch: 64 [4000/10 (20%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 64 [8000/10 (40%)]\tFinal loss: 0.000171\n",
            "Train Epoch: 64 [12000/10 (60%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 64 [16000/10 (80%)]\tFinal loss: 0.000200\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5883, Accuracy: 14676/20000 (73.38%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5884, Accuracy: 14560/20000 (72.80%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6621, Accuracy: 13292/20000 (66.46%)\n",
            "\n",
            "Train Epoch: 65 [0/10 (0%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 65 [4000/10 (20%)]\tFinal loss: 0.000202\n",
            "Train Epoch: 65 [8000/10 (40%)]\tFinal loss: 0.000143\n",
            "Train Epoch: 65 [12000/10 (60%)]\tFinal loss: 0.000121\n",
            "Train Epoch: 65 [16000/10 (80%)]\tFinal loss: 0.000131\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5878, Accuracy: 14610/20000 (73.05%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5877, Accuracy: 14520/20000 (72.60%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6650, Accuracy: 13250/20000 (66.25%)\n",
            "\n",
            "Train Epoch: 66 [0/10 (0%)]\tFinal loss: 0.000177\n",
            "Train Epoch: 66 [4000/10 (20%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 66 [8000/10 (40%)]\tFinal loss: 0.000150\n",
            "Train Epoch: 66 [12000/10 (60%)]\tFinal loss: 0.000194\n",
            "Train Epoch: 66 [16000/10 (80%)]\tFinal loss: 0.000123\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5860, Accuracy: 14876/20000 (74.38%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5849, Accuracy: 14853/20000 (74.27%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6657, Accuracy: 13008/20000 (65.04%)\n",
            "\n",
            "Train Epoch: 67 [0/10 (0%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 67 [4000/10 (20%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 67 [8000/10 (40%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 67 [12000/10 (60%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 67 [16000/10 (80%)]\tFinal loss: 0.000136\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5879, Accuracy: 14913/20000 (74.56%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5866, Accuracy: 14890/20000 (74.45%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6660, Accuracy: 12695/20000 (63.48%)\n",
            "\n",
            "Train Epoch: 68 [0/10 (0%)]\tFinal loss: 0.000247\n",
            "Train Epoch: 68 [4000/10 (20%)]\tFinal loss: 0.000126\n",
            "Train Epoch: 68 [8000/10 (40%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 68 [12000/10 (60%)]\tFinal loss: 0.000177\n",
            "Train Epoch: 68 [16000/10 (80%)]\tFinal loss: 0.000128\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5891, Accuracy: 14891/20000 (74.45%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5882, Accuracy: 14867/20000 (74.33%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6653, Accuracy: 12701/20000 (63.51%)\n",
            "\n",
            "Train Epoch: 69 [0/10 (0%)]\tFinal loss: 0.000149\n",
            "Train Epoch: 69 [4000/10 (20%)]\tFinal loss: 0.000131\n",
            "Train Epoch: 69 [8000/10 (40%)]\tFinal loss: 0.000126\n",
            "Train Epoch: 69 [12000/10 (60%)]\tFinal loss: 0.000169\n",
            "Train Epoch: 69 [16000/10 (80%)]\tFinal loss: 0.000128\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5870, Accuracy: 14916/20000 (74.58%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5860, Accuracy: 14886/20000 (74.43%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6672, Accuracy: 12927/20000 (64.64%)\n",
            "\n",
            "Train Epoch: 70 [0/10 (0%)]\tFinal loss: 0.000154\n",
            "Train Epoch: 70 [4000/10 (20%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 70 [8000/10 (40%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 70 [12000/10 (60%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 70 [16000/10 (80%)]\tFinal loss: 0.000142\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5806, Accuracy: 14945/20000 (74.72%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5772, Accuracy: 14929/20000 (74.64%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6804, Accuracy: 12767/20000 (63.84%)\n",
            "\n",
            "Train Epoch: 71 [0/10 (0%)]\tFinal loss: 0.000121\n",
            "Train Epoch: 71 [4000/10 (20%)]\tFinal loss: 0.000131\n",
            "Train Epoch: 71 [8000/10 (40%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 71 [12000/10 (60%)]\tFinal loss: 0.000139\n",
            "Train Epoch: 71 [16000/10 (80%)]\tFinal loss: 0.000126\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5918, Accuracy: 14488/20000 (72.44%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5925, Accuracy: 14389/20000 (71.94%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6664, Accuracy: 13251/20000 (66.25%)\n",
            "\n",
            "Train Epoch: 72 [0/10 (0%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 72 [4000/10 (20%)]\tFinal loss: 0.000133\n",
            "Train Epoch: 72 [8000/10 (40%)]\tFinal loss: 0.000131\n",
            "Train Epoch: 72 [12000/10 (60%)]\tFinal loss: 0.000168\n",
            "Train Epoch: 72 [16000/10 (80%)]\tFinal loss: 0.000123\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5888, Accuracy: 14702/20000 (73.51%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5885, Accuracy: 14604/20000 (73.02%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6688, Accuracy: 13159/20000 (65.80%)\n",
            "\n",
            "Train Epoch: 73 [0/10 (0%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 73 [4000/10 (20%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 73 [8000/10 (40%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 73 [12000/10 (60%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 73 [16000/10 (80%)]\tFinal loss: 0.000198\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5882, Accuracy: 14865/20000 (74.33%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5876, Accuracy: 14744/20000 (73.72%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6682, Accuracy: 13090/20000 (65.45%)\n",
            "\n",
            "Train Epoch: 74 [0/10 (0%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 74 [4000/10 (20%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 74 [8000/10 (40%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 74 [12000/10 (60%)]\tFinal loss: 0.000135\n",
            "Train Epoch: 74 [16000/10 (80%)]\tFinal loss: 0.000131\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5885, Accuracy: 14884/20000 (74.42%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5881, Accuracy: 14764/20000 (73.82%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6680, Accuracy: 13060/20000 (65.30%)\n",
            "\n",
            "Train Epoch: 75 [0/10 (0%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 75 [4000/10 (20%)]\tFinal loss: 0.000173\n",
            "Train Epoch: 75 [8000/10 (40%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 75 [12000/10 (60%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 75 [16000/10 (80%)]\tFinal loss: 0.000123\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5857, Accuracy: 14906/20000 (74.53%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5843, Accuracy: 14830/20000 (74.15%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6732, Accuracy: 12935/20000 (64.67%)\n",
            "\n",
            "Train Epoch: 76 [0/10 (0%)]\tFinal loss: 0.000131\n",
            "Train Epoch: 76 [4000/10 (20%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 76 [8000/10 (40%)]\tFinal loss: 0.000140\n",
            "Train Epoch: 76 [12000/10 (60%)]\tFinal loss: 0.000151\n",
            "Train Epoch: 76 [16000/10 (80%)]\tFinal loss: 0.000176\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5863, Accuracy: 14984/20000 (74.92%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5851, Accuracy: 14932/20000 (74.66%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6717, Accuracy: 12894/20000 (64.47%)\n",
            "\n",
            "Train Epoch: 77 [0/10 (0%)]\tFinal loss: 0.000147\n",
            "Train Epoch: 77 [4000/10 (20%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 77 [8000/10 (40%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 77 [12000/10 (60%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 77 [16000/10 (80%)]\tFinal loss: 0.000158\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5875, Accuracy: 15015/20000 (75.08%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5866, Accuracy: 14949/20000 (74.75%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6708, Accuracy: 12835/20000 (64.17%)\n",
            "\n",
            "Train Epoch: 78 [0/10 (0%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 78 [4000/10 (20%)]\tFinal loss: 0.000238\n",
            "Train Epoch: 78 [8000/10 (40%)]\tFinal loss: 0.000155\n",
            "Train Epoch: 78 [12000/10 (60%)]\tFinal loss: 0.000137\n",
            "Train Epoch: 78 [16000/10 (80%)]\tFinal loss: 0.000239\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5886, Accuracy: 15012/20000 (75.06%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5876, Accuracy: 14954/20000 (74.77%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6711, Accuracy: 12714/20000 (63.57%)\n",
            "\n",
            "Train Epoch: 79 [0/10 (0%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 79 [4000/10 (20%)]\tFinal loss: 0.000177\n",
            "Train Epoch: 79 [8000/10 (40%)]\tFinal loss: 0.000162\n",
            "Train Epoch: 79 [12000/10 (60%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 79 [16000/10 (80%)]\tFinal loss: 0.000137\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5912, Accuracy: 14983/20000 (74.92%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5908, Accuracy: 14883/20000 (74.42%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6686, Accuracy: 12753/20000 (63.77%)\n",
            "\n",
            "Train Epoch: 80 [0/10 (0%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 80 [4000/10 (20%)]\tFinal loss: 0.000164\n",
            "Train Epoch: 80 [8000/10 (40%)]\tFinal loss: 0.000205\n",
            "Train Epoch: 80 [12000/10 (60%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 80 [16000/10 (80%)]\tFinal loss: 0.000145\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5897, Accuracy: 15025/20000 (75.12%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5887, Accuracy: 14967/20000 (74.83%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6715, Accuracy: 12693/20000 (63.47%)\n",
            "\n",
            "Train Epoch: 81 [0/10 (0%)]\tFinal loss: 0.000153\n",
            "Train Epoch: 81 [4000/10 (20%)]\tFinal loss: 0.000142\n",
            "Train Epoch: 81 [8000/10 (40%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 81 [12000/10 (60%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 81 [16000/10 (80%)]\tFinal loss: 0.000171\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5933, Accuracy: 14951/20000 (74.75%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5935, Accuracy: 14857/20000 (74.28%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6674, Accuracy: 12820/20000 (64.10%)\n",
            "\n",
            "Train Epoch: 82 [0/10 (0%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 82 [4000/10 (20%)]\tFinal loss: 0.000146\n",
            "Train Epoch: 82 [8000/10 (40%)]\tFinal loss: 0.000199\n",
            "Train Epoch: 82 [12000/10 (60%)]\tFinal loss: 0.000136\n",
            "Train Epoch: 82 [16000/10 (80%)]\tFinal loss: 0.000126\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5884, Accuracy: 15116/20000 (75.58%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5859, Accuracy: 15093/20000 (75.47%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6787, Accuracy: 12274/20000 (61.37%)\n",
            "\n",
            "Train Epoch: 83 [0/10 (0%)]\tFinal loss: 0.000194\n",
            "Train Epoch: 83 [4000/10 (20%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 83 [8000/10 (40%)]\tFinal loss: 0.000126\n",
            "Train Epoch: 83 [12000/10 (60%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 83 [16000/10 (80%)]\tFinal loss: 0.000136\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5953, Accuracy: 14944/20000 (74.72%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5956, Accuracy: 14832/20000 (74.16%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6674, Accuracy: 12714/20000 (63.57%)\n",
            "\n",
            "Train Epoch: 84 [0/10 (0%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 84 [4000/10 (20%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 84 [8000/10 (40%)]\tFinal loss: 0.000139\n",
            "Train Epoch: 84 [12000/10 (60%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 84 [16000/10 (80%)]\tFinal loss: 0.000128\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5937, Accuracy: 14985/20000 (74.92%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5936, Accuracy: 14872/20000 (74.36%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6698, Accuracy: 12780/20000 (63.90%)\n",
            "\n",
            "Train Epoch: 85 [0/10 (0%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 85 [4000/10 (20%)]\tFinal loss: 0.000155\n",
            "Train Epoch: 85 [8000/10 (40%)]\tFinal loss: 0.000151\n",
            "Train Epoch: 85 [12000/10 (60%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 85 [16000/10 (80%)]\tFinal loss: 0.000193\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5876, Accuracy: 15049/20000 (75.25%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5854, Accuracy: 15003/20000 (75.02%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6810, Accuracy: 12605/20000 (63.02%)\n",
            "\n",
            "Train Epoch: 86 [0/10 (0%)]\tFinal loss: 0.000134\n",
            "Train Epoch: 86 [4000/10 (20%)]\tFinal loss: 0.000121\n",
            "Train Epoch: 86 [8000/10 (40%)]\tFinal loss: 0.000136\n",
            "Train Epoch: 86 [12000/10 (60%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 86 [16000/10 (80%)]\tFinal loss: 0.000122\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5916, Accuracy: 14903/20000 (74.52%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5909, Accuracy: 14829/20000 (74.14%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6760, Accuracy: 12819/20000 (64.09%)\n",
            "\n",
            "Train Epoch: 87 [0/10 (0%)]\tFinal loss: 0.000170\n",
            "Train Epoch: 87 [4000/10 (20%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 87 [8000/10 (40%)]\tFinal loss: 0.000150\n",
            "Train Epoch: 87 [12000/10 (60%)]\tFinal loss: 0.000139\n",
            "Train Epoch: 87 [16000/10 (80%)]\tFinal loss: 0.000127\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5928, Accuracy: 14876/20000 (74.38%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5925, Accuracy: 14774/20000 (73.87%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6748, Accuracy: 12874/20000 (64.37%)\n",
            "\n",
            "Train Epoch: 88 [0/10 (0%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 88 [4000/10 (20%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 88 [8000/10 (40%)]\tFinal loss: 0.000123\n",
            "Train Epoch: 88 [12000/10 (60%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 88 [16000/10 (80%)]\tFinal loss: 0.000126\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5893, Accuracy: 15012/20000 (75.06%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5877, Accuracy: 14956/20000 (74.78%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6798, Accuracy: 12659/20000 (63.30%)\n",
            "\n",
            "Train Epoch: 89 [0/10 (0%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 89 [4000/10 (20%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 89 [8000/10 (40%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 89 [12000/10 (60%)]\tFinal loss: 0.000301\n",
            "Train Epoch: 89 [16000/10 (80%)]\tFinal loss: 0.000142\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5907, Accuracy: 15013/20000 (75.06%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5897, Accuracy: 14957/20000 (74.78%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6768, Accuracy: 12714/20000 (63.57%)\n",
            "\n",
            "Train Epoch: 90 [0/10 (0%)]\tFinal loss: 0.000138\n",
            "Train Epoch: 90 [4000/10 (20%)]\tFinal loss: 0.000130\n",
            "Train Epoch: 90 [8000/10 (40%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 90 [12000/10 (60%)]\tFinal loss: 0.000160\n",
            "Train Epoch: 90 [16000/10 (80%)]\tFinal loss: 0.000136\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5937, Accuracy: 14993/20000 (74.97%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5938, Accuracy: 14934/20000 (74.67%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6721, Accuracy: 12731/20000 (63.66%)\n",
            "\n",
            "Train Epoch: 91 [0/10 (0%)]\tFinal loss: 0.000143\n",
            "Train Epoch: 91 [4000/10 (20%)]\tFinal loss: 0.000218\n",
            "Train Epoch: 91 [8000/10 (40%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 91 [12000/10 (60%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 91 [16000/10 (80%)]\tFinal loss: 0.000125\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5998, Accuracy: 14923/20000 (74.61%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.6008, Accuracy: 14846/20000 (74.23%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6683, Accuracy: 12439/20000 (62.20%)\n",
            "\n",
            "Train Epoch: 92 [0/10 (0%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 92 [4000/10 (20%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 92 [8000/10 (40%)]\tFinal loss: 0.000164\n",
            "Train Epoch: 92 [12000/10 (60%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 92 [16000/10 (80%)]\tFinal loss: 0.000172\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5976, Accuracy: 14920/20000 (74.60%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5965, Accuracy: 14923/20000 (74.61%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6777, Accuracy: 11833/20000 (59.16%)\n",
            "\n",
            "Train Epoch: 93 [0/10 (0%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 93 [4000/10 (20%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 93 [8000/10 (40%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 93 [12000/10 (60%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 93 [16000/10 (80%)]\tFinal loss: 0.000144\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5981, Accuracy: 14919/20000 (74.59%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5974, Accuracy: 14921/20000 (74.61%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6756, Accuracy: 11935/20000 (59.67%)\n",
            "\n",
            "Train Epoch: 94 [0/10 (0%)]\tFinal loss: 0.000125\n",
            "Train Epoch: 94 [4000/10 (20%)]\tFinal loss: 0.000126\n",
            "Train Epoch: 94 [8000/10 (40%)]\tFinal loss: 0.000132\n",
            "Train Epoch: 94 [12000/10 (60%)]\tFinal loss: 0.000129\n",
            "Train Epoch: 94 [16000/10 (80%)]\tFinal loss: 0.000131\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5968, Accuracy: 15013/20000 (75.06%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5967, Accuracy: 14941/20000 (74.70%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6724, Accuracy: 12344/20000 (61.72%)\n",
            "\n",
            "Train Epoch: 95 [0/10 (0%)]\tFinal loss: 0.000133\n",
            "Train Epoch: 95 [4000/10 (20%)]\tFinal loss: 0.000138\n",
            "Train Epoch: 95 [8000/10 (40%)]\tFinal loss: 0.000135\n",
            "Train Epoch: 95 [12000/10 (60%)]\tFinal loss: 0.000155\n",
            "Train Epoch: 95 [16000/10 (80%)]\tFinal loss: 0.000175\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5932, Accuracy: 15083/20000 (75.42%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5920, Accuracy: 15073/20000 (75.36%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6768, Accuracy: 12267/20000 (61.34%)\n",
            "\n",
            "Train Epoch: 96 [0/10 (0%)]\tFinal loss: 0.000207\n",
            "Train Epoch: 96 [4000/10 (20%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 96 [8000/10 (40%)]\tFinal loss: 0.000128\n",
            "Train Epoch: 96 [12000/10 (60%)]\tFinal loss: 0.000122\n",
            "Train Epoch: 96 [16000/10 (80%)]\tFinal loss: 0.000123\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5919, Accuracy: 15130/20000 (75.65%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5905, Accuracy: 15105/20000 (75.53%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6781, Accuracy: 12393/20000 (61.97%)\n",
            "\n",
            "Train Epoch: 97 [0/10 (0%)]\tFinal loss: 0.000216\n",
            "Train Epoch: 97 [4000/10 (20%)]\tFinal loss: 0.000156\n",
            "Train Epoch: 97 [8000/10 (40%)]\tFinal loss: 0.000173\n",
            "Train Epoch: 97 [12000/10 (60%)]\tFinal loss: 0.000124\n",
            "Train Epoch: 97 [16000/10 (80%)]\tFinal loss: 0.000128\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5963, Accuracy: 15041/20000 (75.20%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5954, Accuracy: 15026/20000 (75.13%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6771, Accuracy: 12139/20000 (60.70%)\n",
            "\n",
            "Train Epoch: 98 [0/10 (0%)]\tFinal loss: 0.000151\n",
            "Train Epoch: 98 [4000/10 (20%)]\tFinal loss: 0.000220\n",
            "Train Epoch: 98 [8000/10 (40%)]\tFinal loss: 0.000166\n",
            "Train Epoch: 98 [12000/10 (60%)]\tFinal loss: 0.000174\n",
            "Train Epoch: 98 [16000/10 (80%)]\tFinal loss: 0.000127\n",
            "\n",
            "Performance on train1 set: Average loss: 0.5945, Accuracy: 15101/20000 (75.50%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.5923, Accuracy: 15133/20000 (75.67%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6835, Accuracy: 11842/20000 (59.21%)\n",
            "\n",
            "Train Epoch: 99 [0/10 (0%)]\tFinal loss: 0.000144\n",
            "Train Epoch: 99 [4000/10 (20%)]\tFinal loss: 0.000137\n",
            "Train Epoch: 99 [8000/10 (40%)]\tFinal loss: 0.000163\n",
            "Train Epoch: 99 [12000/10 (60%)]\tFinal loss: 0.000127\n",
            "Train Epoch: 99 [16000/10 (80%)]\tFinal loss: 0.000130\n",
            "\n",
            "Performance on train1 set: Average loss: 0.6046, Accuracy: 14831/20000 (74.16%)\n",
            "\n",
            "\n",
            "Performance on train2 set: Average loss: 0.6054, Accuracy: 14801/20000 (74.00%)\n",
            "\n",
            "\n",
            "Performance on test set: Average loss: 0.6718, Accuracy: 12132/20000 (60.66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IzSqS9QkC27x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}